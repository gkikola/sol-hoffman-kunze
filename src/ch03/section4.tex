\section{Representation of Transformations by Matrices}

\Exercise1 Let $T$ be the linear operator on $C^2$ defined by
$T(x_1, x_2) = (x_1, 0)$. Let $\mathcal{B}$ be the standard ordered
basis for $C^2$ and let $\mathcal{B}' = \{\alpha_1,\alpha_2\}$ be the
ordered basis defined by $\alpha_1 = (1, i)$, $\alpha_2 = (-i, 2)$.
\begin{enumerate}
\item What is the matrix of $T$ relative to the pair $\mathcal{B}$,
  $\mathcal{B}'$?
  \begin{solution}
    We have
    \begin{equation*}
      T(1, 0) = (1, 0)
      \quad\text{and}\quad
      T(0, 1) = (0, 0).
    \end{equation*}
    Now let
    \begin{equation*}
      P =
      \begin{bmatrix}
        1 & -i \\
        i & 2
      \end{bmatrix}.
    \end{equation*}
    Then
    \begin{equation*}
      P^{-1} =
      \begin{bmatrix}
        2 & i \\
        -i & 1
      \end{bmatrix}
    \end{equation*}
    and we get
    \begin{equation*}
      [(1, 0)]_{\mathcal{B}'}
      = P^{-1}[(1,0)]_{\mathcal{B}}
      =
      \begin{bmatrix}
        2 & i \\
        -i & 1
      \end{bmatrix}
      \begin{bmatrix}
        1 \\ 0
      \end{bmatrix}
      =
      \begin{bmatrix}
        2 \\ -i
      \end{bmatrix}.
    \end{equation*}
    Of course, the zero vector has the same coordinates in every
    basis, so we see that the matrix of $T$ relative to
    $\mathcal{B},\mathcal{B}'$ is
    \begin{equation*}
      [T]_{\mathcal{B}}^{\mathcal{B}'} =
      \begin{bmatrix}
        2 & 0 \\
        -i & 0
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}

\item What is the matrix of $T$ relative to the pair $\mathcal{B}'$,
  $\mathcal{B}$?
  \begin{solution}
    We have
    \begin{equation*}
      T(1,i) = (1,0) \quad\text{and}\quad
      T(-i, 2) = (-i,0).
    \end{equation*}
    So the matrix of $T$ relative to $\mathcal{B}',\mathcal{B}$ is
    \begin{equation*}
      [T]_{\mathcal{B}'}^{\mathcal{B}} =
      \begin{bmatrix}
        1 & -i \\
        0 & 0
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}

\item What is the matrix of $T$ in the ordered basis $\mathcal{B}'$?
  \begin{solution}
    We have
    \begin{equation*}
      [T]_{\mathcal{B}'} = P^{-1}[T]_{\mathcal{B}}P =
      \begin{bmatrix}
        2 & i \\
        -i & 1
      \end{bmatrix}
      \begin{bmatrix}
        1 & 0 \\
        0 & 0
      \end{bmatrix}
      \begin{bmatrix}
        1 & -i \\
        i & 2
      \end{bmatrix} =
      \begin{bmatrix}
        2 & -2i \\
        -i & -1
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}

\item What is the matrix of $T$ in the ordered basis
  $\{\alpha_2, \alpha_1\}$?
  \begin{solution}
    The change-of-basis matrix $P$ such that
    \begin{equation*}
      P[\alpha]_{\{\alpha_2,\alpha_1\}} = [\alpha]_{\mathcal{B}'}
    \end{equation*}
    is given by
    \begin{equation*}
      P =
      \begin{bmatrix}
        0 & 1 \\
        1 & 0
      \end{bmatrix},
    \end{equation*}
    so
    \begin{equation*}
      [T]_{\{\alpha_2,\alpha_1\}}
      = P^{-1}[T]_{\mathcal{B}'}P
      =
      \begin{bmatrix}
        0 & 1 \\
        1 & 0
      \end{bmatrix}
      \begin{bmatrix}
        2 & -2i \\
        -i & -1
      \end{bmatrix}
      \begin{bmatrix}
        0 & 1 \\
        1 & 0
      \end{bmatrix}
      =
      \begin{bmatrix}
        -1 & -i \\
        -2i & 2
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}
\end{enumerate}

\Exercise2 Let $T$ be the linear transformation from $R^3$ into $R^2$
defined by
\begin{equation*}
  T(x_1,x_2,x_3) = (x_1 + x_2, 2x_3 - x_1).
\end{equation*}
\begin{enumerate}
\item If $\mathcal{B}$ is the standard ordered basis for $R^3$ and
  $\mathcal{B}'$ is the standard ordered basis for $R^2$, what is the
  matrix of $T$ relative to the pair $\mathcal{B},\mathcal{B}'$?
  \begin{solution}
    Since
    \begin{equation*}
      T(1, 0, 0) = (1, -1), \quad
      T(0, 1, 0) = (1, 0), \quad\text{and}\quad
      T(0, 0, 1) = (0, 2),
    \end{equation*}
    we see that the matrix of $T$ relative to
    $\mathcal{B},\mathcal{B}'$ is
    \begin{equation*}
      [T]_{\mathcal{B}}^{\mathcal{B}'} =
      \begin{bmatrix}
        1 & 1 & 0 \\
        -1 & 0 & 2
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}

\item If $\mathcal{B} = \{\alpha_1,\alpha_2,\alpha_3\}$ and
  $\mathcal{B}' = \{\beta_1,\beta_2\}$, where
  \begin{multline*}
    \alpha_1 = (1, 0, -1), \quad
    \alpha_2 = (1, 1, 1), \quad
    \alpha_3 = (1, 0, 0), \\
    \beta_1 = (0, 1), \quad
    \beta_2 = (1, 0)
  \end{multline*}
  what is the matrix of $T$ relative to the pair
  $\mathcal{B},\mathcal{B}'$?
  \begin{solution}
    We have
    \begin{align*}
      T\alpha_1 &= (1, -3) = -3\beta_1 + \beta_2, \\
      T\alpha_2 &= (2, 1) = \beta_1 + 2\beta_2, \\
      \intertext{and}
      T\alpha_3 &= (1, -1) = -\beta_1 + \beta_2,
    \end{align*}
    so the corresponding matrix is
    \begin{equation*}
      [T]_{\mathcal{B}}^{\mathcal{B}'} =
      \begin{bmatrix}
        -3 & 1 & -1 \\
        1 & 2 & 1
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}
\end{enumerate}

\Exercise3
\label{exercise:lin-tran:col-space-is-range}
Let $T$ be a linear operator on $F^n$, let $A$ be the matrix of $T$ in
the standard ordered basis for $F^n$, and let $W$ be the subspace of
$F^n$ spanned by the column vectors of $A$. What does $W$ have to do
with $T$?
\begin{solution}
  $W$ is simply the range of $T$, as we will now show.

  Let $\{\epsilon_1,\dots,\epsilon_n\}$ denote the standard ordered
  basis for $F^n$. Note that the $j$th column of $A$ is simply
  $T\epsilon_j$. Take any vector $\alpha$ in $F^n$. Then $\alpha$
  belongs to $W$ if and only if
  \begin{align*}
    \alpha
    &= x_1T\epsilon_1 + x_2T\epsilon_2 + \cdots + x_nT\epsilon_n \\
    &= T(x_1\epsilon_1 + x_2\epsilon_2 + \cdots + x_n\epsilon_n) \\
    &= T(x_1,x_2,\cdots,x_n),
  \end{align*}
  for some vector $(x_1,x_2,\dots,x_n)$ in $F^n$. That is, $\alpha$ is
  in $W$ if and only if $\alpha$ is in the range of $T$.
\end{solution}

\Exercise4 Let $V$ be a two-dimensional vector space over the field
$F$, and let $\mathcal{B}$ be an ordered basis for $V$. If $T$ is a
linear operator on $V$ and
\begin{equation*}
  [T]_{\mathcal{B}} =
  \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}
\end{equation*}
prove that $T^2 - (a + d)T + (ad - bc)I = 0$.
\begin{proof}
  By Theorem~12, the function which assigns a linear operator on $V$
  to its matrix relative to $\mathcal{B}$ is an isomorphism between
  $L(V, V)$ and $F^{2\times 2}$. Theorem~13 shows that this function
  preserves products also. Thus we can operate on $T$ by simply
  performing the corresponding operations on its matrix and vice
  versa. So consider the following computation.
  \begin{multline*}
    [T]_{\mathcal{B}}^2 - (a + d)[T]_{\mathcal{B}} + (ad - bc)I \\
    \begin{aligned}
      &=
      \begin{bmatrix}
        a^2 + bc & ab + bd \\
        ac + cd & bc + d^2
      \end{bmatrix}
      -
      \begin{bmatrix}
        a^2 + ad & ab + bd \\
        ac + cd & ad + d^2
      \end{bmatrix}
      +
      \begin{bmatrix}
        ad - bc & 0 \\
        0 & ad - bc
      \end{bmatrix} \\
      &=
      \begin{bmatrix}
        0 & 0 \\
        0 & 0
      \end{bmatrix}.
    \end{aligned}
  \end{multline*}
  From this we see that $T^2 - (a + d)T + (ad - bc)I = 0$.
\end{proof}

\Exercise5 Let $T$ be the linear operator on $R^3$, the matrix of
which in the standard ordered basis is
\begin{equation*}
  A =
  \begin{bmatrix}
    1 & 2 & 1 \\
    0 & 1 & 1 \\
    -1 & 3 & 4
  \end{bmatrix}.
\end{equation*}
Find a basis for the range of $T$ and a basis for the null space of
$T$.
\begin{solution}
  By Exercise~\ref{exercise:lin-tran:col-space-is-range}, we know that
  the column space of $A$ is the range of $T$. We can find a basis for
  the column space of $A$ by row-reducing its transpose $A^T$ and
  taking the nonzero rows. We get
  \begin{equation*}
    A^T =
    \begin{bmatrix}
      1 & 0 & -1 \\
      2 & 1 & 3 \\
      1 & 1 & 4
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
      1 & 0 & -1 \\
      0 & 1 & 5 \\
      0 & 0 & 0
    \end{bmatrix},
  \end{equation*}
  so a basis for the range of $T$ is given by
  \begin{equation*}
    \{(1, 0, -1), (0, 1, 5)\}.
  \end{equation*}

  For the nullspace, we seek column vectors $X$ for which $AX =
  0$. Row-reducing $A$ gives
  \begin{equation*}
    A =
    \begin{bmatrix}
      1 & 2 & 1 \\
      0 & 1 & 1 \\
      -1 & 3 & 4
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
      1 & 0 & -1 \\
      0 & 1 & 1 \\
      0 & 0 & 0
    \end{bmatrix},
  \end{equation*}
  so $(x_1,x_2,x_3)$ is in the null space of $T$ if and only if
  $x_1 = x_3$ and $x_2 = -x_3$. That is, if and only if
  $(x_1,x_2,x_3)$ has the form $(t,-t,t)$ for some scalar $t$. A basis
  for the null space of $T$ is therefore given by
  \begin{equation*}
    \{ (1,-1,1) \}. \qedhere
  \end{equation*}
\end{solution}

\Exercise6 Let $T$ be the linear operator on $R^2$ defined by
\begin{equation*}
  T(x_1,x_2) = (-x_2, x_1).
\end{equation*}
\begin{enumerate}
\item What is the matrix of $T$ in the standard ordered basis for
  $R^2$?
  \begin{solution}
    Since
    \begin{equation*}
      T(1, 0) = (0, 1) \quad\text{and}\quad T(0, 1) = (-1, 0),
    \end{equation*}
    the matrix of $T$ relative to the standard ordered basis is
    \begin{equation*}
      [T]_{\{\epsilon_1,\epsilon_2\}} =
      \begin{bmatrix}
        0 & -1 \\
        1 & 0
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}

\item What is the matrix of $T$ in the ordered basis
  $\mathcal{B} = \{\alpha_1,\alpha_2\}$, where
  \begin{equation*}
    \alpha_1 = (1,2) \quad\text{and}\quad
    \alpha_2 = (1,-1)?
  \end{equation*}
  \begin{solution}
    The transition matrix $P$ from $\mathcal{B}$ to the standard basis
    is
    \begin{equation*}
      P =
      \begin{bmatrix}
        1 & 1 \\
        2 & -1
      \end{bmatrix},
      \quad\text{with inverse}\quad
      P^{-1} =
      \begin{bmatrix}
        \frac13 & \frac13 \\[3pt]
        \frac23 & -\frac13
      \end{bmatrix}.
    \end{equation*}
    So,
    \begin{equation*}
      [T]_{\mathcal{B}} = P^{-1}[T]_{\{\epsilon_1,\epsilon_2\}}P
      =
      \begin{bmatrix}
        \frac13 & \frac13 \\[3pt]
        \frac23 & -\frac13
      \end{bmatrix}
      \begin{bmatrix}
        0 & -1 \\
        1 & 0
      \end{bmatrix}
      \begin{bmatrix}
        1 & 1 \\
        2 & -1
      \end{bmatrix}
      =
      \begin{bmatrix}
        -\frac13 & \frac23 \\[3pt]
        -\frac53 & \frac13
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}

\item Prove that for every real number $c$ the operator $(T - cI)$ is
  invertible.
  \begin{proof}
    Fix $c$ in $R$ and let $U = T - cI$. If $\mathcal{B}$ is the
    standard ordered basis for $R^2$, then
    \begin{equation*}
      [U]_{\mathcal{B}} = [T]_{\mathcal{B}} - cI
      =
      \begin{bmatrix}
        0 & -1 \\
        1 & 0
      \end{bmatrix}
      - c
      \begin{bmatrix}
        1 & 0 \\
        0 & 1
      \end{bmatrix}
      =
      \begin{bmatrix}
        -c & -1 \\
        1 & -c
      \end{bmatrix}.
    \end{equation*}
    Since $c^2 + 1$ must be nonzero, it is easily verified that
    \begin{equation*}
      \begin{bmatrix}
        -\frac{c}{c^2 + 1} & \frac1{c^2 + 1} \\[3pt]
        -\frac1{c^2 + 1} & -\frac{c}{c^2 + 1}
      \end{bmatrix}
      = -\frac1{c^2 + 1}
      \begin{bmatrix}
        c & -1 \\
        1 & c
      \end{bmatrix}
    \end{equation*}
    is an inverse for $[U]_{\mathcal{B}}$. It follows that $U$ is
    invertible, as required.
  \end{proof}

\item Prove that if $\mathcal{B}$ is any ordered basis for $R^2$ and
  $[T]_{\mathcal{B}} = A$, then $A_{12}A_{21}\neq0$.
  \begin{proof}
    Let $\mathcal{B} = \{\alpha_1,\alpha_2\}$ where
    \begin{equation*}
      \alpha_1 = (a,c) \quad\text{and}\quad
      \alpha_2 = (b,d)
    \end{equation*}
    so that
    \begin{equation*}
      [T]_{\mathcal{B}}
      = P^{-1}[T]_{\{\epsilon_1,\epsilon_2\}}P,
    \end{equation*}
    where
    \begin{equation*}
      P =
      \begin{bmatrix}
        a & b \\
        c & d
      \end{bmatrix}.
    \end{equation*}
    Since $P$ is invertible, we know from
    Exercise~\ref{exercise:lin-eq:2-by-2-inv-crit} that
    $ad - bc\neq0$, and through direct computation we can find that
    \begin{equation*}
      A = [T]_{\mathcal{B}} =
      \begin{bmatrix}
        \frac{d}{ad - bc} & -\frac{b}{ad - bc} \\[3pt]
        -\frac{c}{ad - bc} & \frac{a}{ad - bc}
      \end{bmatrix}
      \begin{bmatrix}
        0 & -1 \\
        1 & 0
      \end{bmatrix}
      \begin{bmatrix}
        a & b \\
        c & d
      \end{bmatrix}
      =
      \begin{bmatrix}
        \frac{-cd - ab}{ad - bc} & \frac{-d^2 - b^2}{ad - bc} \\[3pt]
        \frac{c^2 + a^2}{ad - bc} & \frac{cd + ab}{ad - bc}
      \end{bmatrix}.
    \end{equation*}
    Now, if $A_{12} = 0$, then $b^2 + d^2 = 0$ so that we must have
    $b = d = 0$. But this is impossible since $ad -
    bc\neq0$. Similarly if $A_{21} = 0$ then $a = c = 0$ which is
    again a contradiction. So we find that $A_{12}$ and $A_{21}$ are
    each nonzero, and their product must be nonzero also.
  \end{proof}
\end{enumerate}

\Exercise7 Let $T$ be the linear operator on $R^3$ defined by
\begin{equation*}
  T(x_1,x_2,x_3)
  = (3x_1 + x_3, -2x_1 + x_2, -x_1 + 2x_2 + 4x_3).
\end{equation*}
\begin{enumerate}
\item What is the matrix of $T$ in the standard ordered basis for
  $R^3$?
  \begin{solution}
    Since
    \begin{align*}
      T(1,0,0) &= (3, -2, -1), \\
      T(0,1,0) &= (0, 1, 2), \\
      T(0,0,1) &= (1, 0, 4),
    \end{align*}
    the matrix of $T$ in the standard ordered basis is
    \begin{equation*}
      [T]_{\{\epsilon_1,\epsilon_2,\epsilon_3\}} =
      \begin{bmatrix}
        3 & 0 & 1 \\
        -2 & 1 & 0 \\
        -1 & 2 & 4
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}

\item What is the matrix of $T$ in the ordered basis
  \begin{equation*}
    \{\alpha_1,\alpha_2,\alpha_3\}
  \end{equation*}
  where $\alpha_1 = (1,0,1)$, $\alpha_2 = (-1,2,1)$, and
  $\alpha_3 = (2,1,1)$?
  \begin{solution}
    The matrix of $T$ in this basis is
    \begin{equation*}
      [T]_{\{\alpha_1,\alpha_2,\alpha_3\}}
      = P^{-1}[T]_{\{\epsilon_1,\epsilon_2,\epsilon_3\}}P,
    \end{equation*}
    where
    \begin{equation*}
      P =
      \begin{bmatrix}
        1 & -1 & 2 \\
        0 & 2 & 1 \\
        1 & 1 & 1
      \end{bmatrix}.
    \end{equation*}
    So
    \begin{align*}
      [T]_{\{\alpha_1,\alpha_2,\alpha_3\}}
      &=
      \begin{bmatrix}
        -\frac14 & -\frac34 & \frac54 \\[3pt]
        -\frac14 & \frac14 & \frac14 \\[3pt]
        \frac12 & \frac12 & -\frac12
      \end{bmatrix}
      \begin{bmatrix}
        3 & 0 & 1 \\
        -2 & 1 & 0 \\
        -1 & 2 & 4
      \end{bmatrix}
      \begin{bmatrix}
        1 & -1 & 2 \\
        0 & 2 & 1 \\
        1 & 1 & 1
      \end{bmatrix} \\[3pt]
      &=
      \begin{bmatrix}
        \frac{17}4 & \frac{35}4 & \frac{11}2 \\[3pt]
        -\frac34 & \frac{15}4 & -\frac32 \\[3pt]
        -\frac12 & -\frac72 & 0
      \end{bmatrix}. \qedhere
    \end{align*}
  \end{solution}

\item Prove that $T$ is invertible and give a rule for $T^{-1}$ like
  the one which defines $T$.
  \begin{proof}
    We already saw that the matrix $A$ of $T$ relative to the standard
    ordered basis of $R^3$ is
    \begin{equation*}
      A =
      \begin{bmatrix}
        3 & 0 & 1 \\
        -2 & 1 & 0 \\
        -1 & 2 & 4
      \end{bmatrix}.
    \end{equation*}
    Using the same methods as we used in Chapter~1, we can see that
    $A$ is invertible and
    \begin{equation*}
      A^{-1} =
      \begin{bmatrix}
        \frac49 & \frac29 & -\frac19 \\[3pt]
        \frac89 & \frac{13}9 & -\frac29 \\[3pt]
        -\frac13 & -\frac23 & \frac13
      \end{bmatrix}.
    \end{equation*}
    This implies that $T$ is invertible and
    \begin{multline*}
      T^{-1}(x_1,x_2,x_3) \\
      = \left(
        \frac49x_1 + \frac29x_2 - \frac19x_3,
        \frac89x_1 + \frac{13}9x_2 - \frac29x_3,
        -\frac13x_1 - \frac23x_2 + \frac13x_3
      \right).
    \end{multline*}
  \end{proof}
\end{enumerate}

\Exercise8 Let $\theta$ be a real number. Prove that the following two
matrices are similar over the field of complex numbers:
\begin{equation*}
  \begin{bmatrix}
    \cos\theta & -\sin\theta \\
    \sin\theta & \cos\theta
  \end{bmatrix},
  \quad
  \begin{bmatrix}
    e^{i\theta} & 0 \\
    0 & e^{-i\theta}
  \end{bmatrix}
\end{equation*}
\begin{proof}
  Let $T$ be the linear operator on $C^2$ which is represented by the
  first matrix in the standard ordered basis. Let
  \begin{equation*}
    \alpha_1 = (1, -i) \quad\text{and}\quad \alpha_2 = (-i, 1),
  \end{equation*}
  so that $\mathcal{B} = \{\alpha_1,\alpha_2\}$ is an ordered basis
  for $C^2$. Then
  \begin{align*}
    T\alpha_1
    &= (\cos\theta + i\sin\theta, \sin\theta - i\cos\theta)
      = (e^{i\theta}, -ie^{i\theta}) = e^{i\theta}\alpha_1
    \\\intertext{and}
    T\alpha_2
    &= (-i\cos\theta - \sin\theta, -i\sin\theta + \cos\theta)
      = (-ie^{-i\theta}, e^{-i\theta}) = e^{-i\theta}\alpha_2.
  \end{align*}
  Thus the second matrix represents $T$ in the ordered basis
  $\mathcal{B}$. By Theorem~14, the two matrices are similar.
\end{proof}

\Exercise9
\label{exercise:lin-tran:S-T-same-mat-diff-basis}
Let $V$ be a finite-dimensional vector space over the field $F$ and
let $S$ and $T$ be linear operators on $V$. We ask: When do there
exist ordered bases $\mathcal{B}$ and $\mathcal{B}'$ for $V$ such that
$[S]_{\mathcal{B}} = [T]_{\mathcal{B}'}$? Prove that such bases exist
if and only if there is an invertible linear operator $U$ on $V$ such
that $T = USU^{-1}$.
\begin{proof}
  Assume such bases exist, so that
  $[S]_{\mathcal{B}} = [T]_{\mathcal{B}'}$. Let $U$ be the operator
  which carries $\mathcal{B}$ onto $\mathcal{B}'$. Then by Theorem~14,
  we have
  \begin{equation*}
    [S]_{\mathcal{B}} = [T]_{\mathcal{B}'}
    = [U]_{\mathcal{B}}^{-1}[T]_{\mathcal{B}}[U]_{\mathcal{B}}
    = [U^{-1}TU]_{\mathcal{B}}.
  \end{equation*}
  Since $S$ and $U^{-1}TU$ have the same matrix relative to
  $\mathcal{B}$, it follows by Theorem~12 that $S = U^{-1}TU$. Thus we
  have shown that there is an invertible operator $U$ with
  $T = USU^{-1}$.

  Conversely, assume that $T = USU^{-1}$ for some invertible $U$ and
  let $\mathcal{B}$ be any ordered basis for $V$. Let $\mathcal{B}'$
  be the image of $\mathcal{B}$ under $U$. Then, again by Theorem~14,
  we have
  \begin{equation*}
    [T]_{\mathcal{B}'}
    = [U]_{\mathcal{B}}^{-1}[T]_{\mathcal{B}}[U]_{\mathcal{B}}
    = [U^{-1}TU]_{\mathcal{B}}
    = [S]_{\mathcal{B}}.
  \end{equation*}
  Thus the proof is complete.
\end{proof}

\Exercise{10} We have seen that the linear operator $T$ on $R^2$
defined by $T(x_1,x_2) = (x_1,0)$ is represented in the standard
ordered basis by the matrix
\begin{equation*}
  A =
  \begin{bmatrix}
    1 & 0 \\
    0 & 0
  \end{bmatrix}.
\end{equation*}
This operator satisfies $T^2 = T$. Prove that if $S$ is a linear
operator on $R^2$ such that $S^2 = S$, then $S = 0$, or $S = I$, or
there is an ordered basis $\mathcal{B}$ for $R^2$ such that
$[S]_{\mathcal{B}} = A$ (above).
\begin{proof}
  Assume that $S$ is such that $S^2 = S$. Certainly $0^2 = 0$ and
  $I^2 = I$, so if $S = 0$ or $S = I$ there is nothing left to
  prove. We will therefore assume that $S\neq0$ and $S\neq I$. Since
  $S\neq0$, there is nonzero $\alpha_1$ in the range of $S$. So
  $S\beta_1 = \alpha_1$ for some $\beta_1$ in $R^2$ and we have
  \begin{equation}
    \label{eq:lin-tran:S2-equals-S-fixed-point}
    S\alpha_1 = S(S\beta_1) = S\beta_1 = \alpha_1.
  \end{equation}
  On the other hand, since $S\neq I$, there exist distinct $\beta_2$
  and $\beta_3$ such that
  \begin{equation*}
    S\beta_2 = \beta_3.
  \end{equation*}
  Applying $S$ to both sides, we get $S^2\beta_2 = S\beta_3$ or
  $S\beta_2 = S\beta_3$. Now letting $\alpha_2 = \beta_3 - \beta_2$,
  we get
  \begin{equation}
    \label{eq:lin-tran:S2-equals-S-null-space}
    S\alpha_2 = S\beta_3 - S\beta_2 = 0.
  \end{equation}

  Next, if $c_1\alpha_1 + c_2\alpha_2 = 0$ for scalars $c_1$ and
  $c_2$, then
  \begin{equation*}
    c_1S\alpha_1 + c_2S\alpha_2 = 0.
  \end{equation*}
  Substituting equations \eqref{eq:lin-tran:S2-equals-S-fixed-point}
  and \eqref{eq:lin-tran:S2-equals-S-null-space}, we see
  $c_1\alpha_1 = 0$, which implies that $c_1 = 0$ since $\alpha_1$ is
  nonzero. So $c_2\alpha_2 = 0$ and we must have $c_2 = 0$ since
  $\alpha_2$ is also nonzero. Thus the set
  $\mathcal{B} = \{\alpha_1,\alpha_2\}$ is a set of two linearly
  independent vectors in $R^2$. Therefore $\mathcal{B}$ spans $R^2$
  and is a basis.

  Finally, we see that \eqref{eq:lin-tran:S2-equals-S-fixed-point} and
  \eqref{eq:lin-tran:S2-equals-S-null-space} together imply that
  \begin{equation*}
    [S]_{\mathcal{B}} =
    \begin{bmatrix}
      1 & 0 \\
      0 & 0
    \end{bmatrix}
    = A,
  \end{equation*}
  so the proof is complete.
\end{proof}

\Exercise{11} Let $W$ be the space of all $n\times1$ column matrices
over a field $F$. If $A$ is an $n\times n$ matrix over $F$, then $A$
defines a linear operator $L_A$ on $W$ through left multiplication:
$L_A(X) = AX$. Prove that every linear operator on $W$ is left
multiplication by some $n\times n$ matrix, i.e., is $L_A$ for some
$A$.

Now suppose $V$ is an $n$-dimensional vector space over the field $F$,
and let $\mathcal{B}$ be an ordered basis for $V$. For each $\alpha$
in $V$, define $U\alpha = [\alpha]_{\mathcal{B}}$. Prove that $U$ is
an isomorphism of $V$ onto $W$. If $T$ is a linear operator on $V$,
then $UTU^{-1}$ is a linear operator on $W$. Accordingly, $UTU^{-1}$
is left multiplication by some $n\times n$ matrix $A$. What is $A$?
\begin{solution}
  Let $\mathcal{C}$ be the standard ordered basis for
  $W = F^{n\times1}$, i.e. the $j$th vector in the basis has a $1$ in
  its $j$th row and all other entries zero. Then if $S$ is any linear
  operator on $W$, we may take the $n\times n$ matrix $A$ to be
  \begin{equation*}
    A = [S]_{\mathcal{C}}.
  \end{equation*}
  Then $L_A$ and $S$ have the same matrix relative to the ordered
  basis $\mathcal{C}$, and therefore $L_A = S$. Therefore every linear
  operator on $W$ is left multiplication by some $n\times n$ matrix.

  Now let $V$ be an $n$-dimensional vector space over $F$ and
  $\mathcal{B} = \{\alpha_1,\dots,\alpha_n\}$ an ordered basis for
  $V$. Define $U\alpha = [\alpha]_{\mathcal{B}}$, so that $U$ is a map
  of $V$ into $W$. We will show that $U$ is an isomorphism.

  First, for any $\beta_1,\beta_2$ in $V$ and scalar $c$ in $F$, we
  have
  \begin{align*}
    U(c\beta_1 + \beta_2)
    &= [c\beta_1 + \beta_2]_{\mathcal{B}} \\
    &= c[\beta_1]_{\mathcal{B}} + [\beta_2]_{\mathcal{B}} \\
    &= cU\beta_1 + U\beta_2,
  \end{align*}
  and $U$ is a linear transformation.

  Now choose $Y$ in $W$, where
  \begin{equation*}
    Y =
    \begin{bmatrix}
      y_1 \\
      y_2 \\
      \vdots \\
      y_n
    \end{bmatrix}.
  \end{equation*}
  Define
  \begin{equation*}
    \beta = y_1\alpha_1 + y_2\alpha_2 + \cdots + y_n\alpha_n.
  \end{equation*}
  Then
  \begin{equation*}
    U\beta = [\beta]_{\mathcal{B}} = Y.
  \end{equation*}
  So $Y$ is in the range of $U$, and $U$ is therefore onto. By
  Theorem~9, $U$ is invertible and therefore an isomorphism of $V$
  onto $W$.

  Finally, if $T$ is a linear operator on $V$, then $UTU^{-1}$ is a
  linear operator on $W$ and hence $UTU^{-1} = L_A$ for some
  $n\times n$ matrix $A$. Choose any $X$ in $W$, with
  \begin{equation*}
    X =
    \begin{bmatrix}
      x_1 \\ x_2 \\ \vdots \\ x_n
    \end{bmatrix}.
  \end{equation*}
  Then
  \begin{align*}
    UTU^{-1}(X)
    &= UT(x_1\alpha_1 + x_2\alpha_2 + \cdots + x_n\alpha_n) \\
    &= U(x_1T\alpha_1 + x_2T\alpha_2 + \cdots + x_nT\alpha_n) \\
    &= x_1[T\alpha_1]_{\mathcal{B}} + x_2[T\alpha_2]_{\mathcal{B}}
      + \cdots + x_n[T\alpha_n]_{\mathcal{B}} \\
    &= x_1[T]_{\mathcal{B}}[\alpha_1]_{\mathcal{B}}
      + x_2[T]_{\mathcal{B}}[\alpha_2]_{\mathcal{B}}
      + \cdots
      + x_n[T]_{\mathcal{B}}[\alpha_n]_{\mathcal{B}} \\
    &= [T]_{\mathcal{B}}(x_1U\alpha_1 + x_2U\alpha_2
      + \cdots + x_nU\alpha_n) \\
    &= [T]_{\mathcal{B}}X.
  \end{align*}
  We see that $A = [T]_{\mathcal{B}}$, so
  $UTU^{-1} = L_{[T]_{\mathcal{B}}}$.
\end{solution}

\Exercise{12} Let $V$ be an $n$-dimensional vector space over the
field $F$, and let
\begin{equation*}
  \mathcal{B} = \{\alpha_1,\dots,\alpha_n\}
\end{equation*}
be an ordered basis for $V$.
\begin{enumerate}
\item \label{itm:lin-tran:T-alphaj-eq-alpha-jp1} According to
  Theorem~1, there is a unique linear operator $T$ on $V$ such that
  \begin{equation*}
    T\alpha_j = \alpha_{j+1}, \quad
    j = 1, \dots, n-1, \quad
    T\alpha_n = 0.
  \end{equation*}
  What is the matrix $A$ of $T$ in the ordered basis $\mathcal{B}$?
  \begin{solution}
    For each $j = 1, \dots, n-1$, we have $A_{j+1,j} = 1$ and all
    other entries $0$. That is,
    \begin{equation*}
      A = [T]_{\mathcal{B}} =
      \begin{bmatrix}
        0 & 0 & 0 & \cdots & 0 & 0 \\
        1 & 0 & 0 & \cdots & 0 & 0 \\
        0 & 1 & 0 & \cdots & 0 & 0 \\
        0 & 0 & 1 & \cdots & 0 & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & 0 & \cdots & 1 & 0
      \end{bmatrix}. \qedhere
    \end{equation*}
  \end{solution}

\item Prove that $T^n = 0$ but $T^{n-1}\neq0$.
  \begin{proof}
    Since $T^{n-1}\alpha_1 = \alpha_n \neq 0$, we see that
    $T^{n-1}\neq0$. On the other hand,
    \begin{equation*}
      T^n\alpha_1 = T(T^{n-1}\alpha_1) = T\alpha_n = 0,
    \end{equation*}
    and we can similarly see that $T^n\alpha_j = 0$ for all $j$ with
    $1\leq j\leq n$. Since any vector $\alpha$ can be written as a
    linear combination of the basis vectors $\alpha_1,\dots,\alpha_n$,
    it follows by the linearity of $T$ that $T^n\alpha = 0$. Therefore
    $T^n = 0$.
  \end{proof}

\item Let $S$ be any linear operator on $V$ such that $S^n = 0$ but
  $S^{n-1}\neq0$. Prove that there is an ordered basis $\mathcal{B}'$
  for $V$ such that the matrix of $S$ in the ordered basis
  $\mathcal{B}'$ is the matrix $A$ of part
  \ref{itm:lin-tran:T-alphaj-eq-alpha-jp1}.
  \begin{proof}
    Since $S^{n-1}\neq0$, there is a nonzero $\beta_1$ in $V$ such
    that $S^{n-1}\beta_1$ is nonzero. Now, for each $j$ with
    $2\leq j\leq n$, define
    \begin{equation*}
      \beta_j = S^{j-1}\beta_1.
    \end{equation*}
    Let $\mathcal{B}' = \{\beta_1,\dots,\beta_n\}$.

    Now suppose $c_1,c_2,\dots,c_n$ are scalars in $F$ such that
    \begin{equation*}
      c_1\beta_1 + c_2\beta_2 + \cdots + c_n\beta_n = 0.
    \end{equation*}
    Taking $S^{n-1}$ of both sides gives $c_1\beta_n = 0$, which
    implies that $c_1 = 0$. So
    \begin{equation*}
      c_2\beta_2 + c_3\beta_3 + \cdots + c_n\beta_n = 0,
    \end{equation*}
    and we can take $S^{n-2}$ of both sides to get $c_2\beta_n = 0$,
    implying that $c_2$ is zero. More generally, assuming that
    $c_1,\dots,c_k$ are all zero for some $k$ with $1\leq k\leq n-1$,
    we have
    \begin{equation*}
      c_{k+1}\beta_{k+1} + \cdots + c_n\beta_n = 0.
    \end{equation*}
    Taking $S^{n-k-1}$ of both sides (in the case where $k = n-1$, we
    take $S^0 = I$) then gives $c_{k+1}\beta_n = 0$, so that
    $c_{k+1} = 0$. Therefore $c_1 = c_2 = \cdots = c_n = 0$ and
    $\mathcal{B}'$ is linearly independent. Since $\dim V = n$ and
    $\mathcal{B}'$ is a linearly independent set of $n$ vectors in
    $V$, it follows that $\mathcal{B}'$ is a basis for $V$.

    Finally, we have defined $\beta_1,\dots,\beta_n$ so that
    \begin{equation*}
      S\beta_j = \beta_{j+1}, \quad
      j = 1, \dots, n - 1, \quad
      S\beta_n = 0.
    \end{equation*}
    Therefore we have $[S]_{\mathcal{B}'} = A$.
  \end{proof}

\item Prove that if $M$ and $N$ are $n\times n$ matrices over $F$ such
  that $M^n = N^n = 0$ but $M^{n-1}\neq 0\neq N^{n-1}$, then $M$ and
  $N$ are similar.
  \begin{proof}
    Let $P$ and $Q$ be the linear operators on $V$ whose matrices
    relative to $\mathcal{B}$ are, respectively, $M$ and $N$. Then
    $P^n = Q^n = 0$ but $P^{n-1}\neq0\neq Q^{n-1}$. We have shown
    above that there are bases $\mathcal{C}$ and $\mathcal{C}'$ for
    $V$ such that
    \begin{equation*}
      [P]_{\mathcal{C}} = A = [Q]_{\mathcal{C}'}.
    \end{equation*}
    By Exercise~\ref{exercise:lin-tran:S-T-same-mat-diff-basis}, there
    is an invertible linear transformation $U$ such that
    $Q = UPU^{-1}$. Therefore
    \begin{equation*}
      N = [Q]_{\mathcal{B}}
      = [U]_{\mathcal{B}}[P]_{\mathcal{B}}[U]_{\mathcal{B}}^{-1}
      = [U]_{\mathcal{B}}M[U]_{\mathcal{B}}^{-1}
    \end{equation*}
    and we see that $M$ and $N$ are similar matrices.
  \end{proof}
\end{enumerate}
