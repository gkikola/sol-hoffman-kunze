\section{Linear Functionals}

\Exercise1 In $R^3$, let $\alpha_1 = (1, 0, 1)$,
$\alpha_2 = (0, 1, -2)$, $\alpha_3 = (-1, -1, 0)$.
\begin{enumerate}
\item If $f$ is a linear functional on $R^3$ such that
  \begin{equation*}
    f(\alpha_1) = 1, \quad
    f(\alpha_2) = -1, \quad
    f(\alpha_3) = 3,
  \end{equation*}
  and if $\alpha = (a, b, c)$, find $f(\alpha)$.
  \begin{solution}
    Suppose $f(x_1,x_2,x_3) = c_1x_1 + c_2x_2 + c_3x_3$. Then
    \begin{alignat*}{5}
      f(\alpha_1)
      &{}={}& c_1 && &{}+{}& c_3 &{}={}& 1 &, \\
      f(\alpha_2)
      &{}={}& && c_2 &{}-{}& 2c_3 &{}={}& -1 &, \\
      f(\alpha_3)
      &{}={}& -c_1 &{}-{}& c_2 && &{}={}& 3 &.
    \end{alignat*}
    Row-reducing the augmented matrix for this system gives
    \begin{equation*}
      \begin{bmatrix}
        1 & 0 & 1 & 1 \\
        0 & 1 & -2 & -1 \\
        -1 & -1 & 0 & 3
      \end{bmatrix}
      \rightarrow
      \begin{bmatrix}
        1 & 0 & 0 & 4 \\
        0 & 1 & 0 & -7 \\
        0 & 0 & 1 & -3
      \end{bmatrix},
    \end{equation*}
    so $c_1 = 4$, $c_2 = -7$, and $c_3 = -3$. Therefore
    \begin{equation*}
      f(\alpha) = 4a - 7b - 3c. \qedhere
    \end{equation*}
  \end{solution}

\item Describe explicitly a linear functional $f$ on $R^3$ such that
  \begin{equation*}
    f(\alpha_1) = f(\alpha_2) = 0 \quad\text{but}\quad
    f(\alpha_3) \neq 0.
  \end{equation*}
  \begin{solution}
    For example, suppose $f(\alpha_1) = f(\alpha_2) = 0$ but
    $f(\alpha_3) = 1$. As above, this leads to a system of linear
    equations having augmented matrix
    \begin{equation*}
      \begin{bmatrix}
        1 & 0 & 1 & 0 \\
        0 & 1 & -2 & 0 \\
        -1 & -1 & 0 & 1
      \end{bmatrix}
      \rightarrow
      \begin{bmatrix}
        1 & 0 & 0 & 1 \\
        0 & 1 & 0 & -2 \\
        0 & 0 & 1 & -1
      \end{bmatrix}.
    \end{equation*}
    So we may write
    \begin{equation*}
      f(x_1,x_2,x_3) = x_1 - 2x_2 - x_3. \qedhere
    \end{equation*}
  \end{solution}

\item Let $f$ be any linear functional such that
  \begin{equation*}
    f(\alpha_1) = f(\alpha_2) = 0 \quad\text{and}\quad
    f(\alpha_3) \neq 0.
  \end{equation*}
  If $\alpha = (2, 3, -1)$, show that $f(\alpha)\neq0$.
  \begin{solution}
    By inspection, we see that
    \begin{equation*}
      \alpha = -\alpha_1 - 3\alpha_3.
    \end{equation*}
    Therefore
    \begin{align*}
      f(\alpha)
      &= f(-\alpha_1 - 3\alpha_3) \\
      &= -f(\alpha_1) - 3f(\alpha_3) \\
      &= -3f(\alpha_3) \neq 0. \qedhere
    \end{align*}
  \end{solution}
\end{enumerate}

\Exercise2 Let $\mathcal{B} = \{\alpha_1,\alpha_2,\alpha_3\}$ be the
basis for $C^3$ defined by
\begin{equation*}
  \alpha_1 = (1,0,-1), \quad
  \alpha_2 = (1,1,1), \quad
  \alpha_3 = (2,2,0).
\end{equation*}
Find the dual basis of $\mathcal{B}$.
\begin{solution}
  Let $\{f_1,f_2,f_3\}$ be the dual basis of $\mathcal{B}$, and let
  \begin{equation*}
    P =
    \begin{bmatrix}
      1 & 1 & 2 \\
      0 & 1 & 2 \\
      -1 & 1 & 0
    \end{bmatrix},
  \end{equation*}
  so that $P$ is the transition matrix from $\mathcal{B}$ to the
  standard ordered basis of $C^3$. We find
  \begin{equation*}
    P^{-1} =
    \begin{bmatrix}
      1 & -1 & 0 \\[3pt]
      1 & -1 & 1 \\[3pt]
      -\frac12 & 1 & -\frac12
    \end{bmatrix}.
  \end{equation*}
  So, given a vector $\alpha = (x_1,x_2,x_3)$ in $C_3$, we can write
  \begin{equation*}
    [\alpha]_{\mathcal{B}} = P^{-1}
    \begin{bmatrix}
      x_1 \\ x_2 \\ x_3
    \end{bmatrix}.
  \end{equation*}
  Therefore
  \begin{equation*}
    f_1(x_1,x_2,x_3) =
    \begin{bmatrix}
      1 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & -1 & 0 \\[3pt]
      1 & -1 & 1 \\[3pt]
      -\frac12 & 1 & -\frac12
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\ x_2 \\ x_3
    \end{bmatrix}
    = x_1 - x_2.
  \end{equation*}
  Similarly, we get
  \begin{equation*}
    f_2(x_1,x_2,x_3) =
    \begin{bmatrix}
      0 & 1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & -1 & 0 \\[3pt]
      1 & -1 & 1 \\[3pt]
      -\frac12 & 1 & -\frac12
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\ x_2 \\ x_3
    \end{bmatrix}
    = x_1 - x_2 + x_3,
  \end{equation*}
  and
  \begin{equation*}
    f_3(x_1,x_2,x_3) =
    \begin{bmatrix}
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & -1 & 0 \\[3pt]
      1 & -1 & 1 \\[3pt]
      -\frac12 & 1 & -\frac12
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\ x_2 \\ x_3
    \end{bmatrix}
    = -\frac12x_1 + x_2 - \frac12x_3.
    \qedhere
  \end{equation*}
\end{solution}

\Exercise3
\label{exercise:lin-tran:trace-AB-eq-trace-BA}
If $A$ and $B$ are $n\times n$ matrices over the field $F$, show that
$\trace(AB) = \trace(BA)$. Now show that similar matrices have the
same trace.
\begin{proof}
  We may directly compute
  \begin{align*}
    \tr(AB)
    &= \sum_{i=1}^n(AB)_{ii} \\
    &= \sum_{i=1}^n\sum_{j=1}^nA_{ij}B_{ji}
    = \sum_{j=1}^n\sum_{i=1}^nB_{ji}A_{ij} \\
    &= \sum_{j=1}^n(BA)_{jj} \\
    &= \tr(BA).
  \end{align*}
  So $\tr(AB) = \tr(BA)$.

  Next, suppose $A$ and $B$ are similar, and let $P$ be an invertible
  $n\times n$ matrix such that $B = P^{-1}AP$. Using the fact that was
  proven above, we get
  \begin{align*}
    \tr(B)
    &= \tr(P^{-1}AP) \\
    &= \tr((P^{-1}A)P) \\
    &= \tr(P(P^{-1}A)) \\
    &= \tr((PP^{-1})A) \\
    &= \tr(A).
  \end{align*}
  This shows that similar matrices have the same trace.
\end{proof}

\Exercise4 Let $V$ be the vector space of all polynomial functions $p$
from $R$ into $R$ which have degree $2$ or less:
\begin{equation*}
  p(x) = c_0 + c_1x + c_2x^2.
\end{equation*}
Define three linear functionals on $V$ by
\begin{equation*}
  f_1(p) = \int_0^1p(x)\,dx, \quad
  f_2(p) = \int_0^2p(x)\,dx, \quad
  f_3(p) = \int_0^{-1}p(x)\,dx.
\end{equation*}
Show that $\{f_1,f_2,f_3\}$ is a basis for $V^*$ by exhibiting the
basis for $V$ of which it is the dual.
\begin{solution}
  First we evaluate,
  \begin{align*}
    f_1(p) &= \left(c_0x + \frac12c_1x^2 + \frac13c_2x^3\right)\Bigg|_0^1
    = c_0 + \frac12c_1 + \frac13c_2, \\
    f_2(p) &= \left(c_0x + \frac12c_1x^2 + \frac13c_2x^3\right)\Bigg|_0^2
    = 2c_0 + 2c_1 + \frac83c_2, \\
    f_3(p) &= \left(c_0x + \frac12c_1x^2 + \frac13c_2x^3\right)\Bigg|_0^{-1}
    = -c_0 + \frac12c_1 - \frac13c_2.
  \end{align*}
  Now, let $\{p_1,p_2,p_3\}$ be the basis for $V$ of which
  $\{f_1,f_2,f_3\}$ is the dual. To determine $p_i$, we want to find
  values for the coefficients $c_1$, $c_2$, and $c_3$ so that
  $f_i(p_i) = 1$ and $f_j(p_i) = 0$ for $j\neq i$. This gives three
  systems of linear equations, having augmented matrices
  \begin{equation*}
    \begin{bmatrix}
      1 & \frac12 & \frac13 & 1 \\[3pt]
      2 & 2 & \frac83 & 0 \\[3pt]
      -1 & \frac12 & -\frac13 & 0
    \end{bmatrix}, \quad
    \begin{bmatrix}
      1 & \frac12 & \frac13 & 0 \\[3pt]
      2 & 2 & \frac83 & 1 \\[3pt]
      -1 & \frac12 & -\frac13 & 0
    \end{bmatrix}, \quad\text{and}\quad
    \begin{bmatrix}
      1 & \frac12 & \frac13 & 0 \\[3pt]
      2 & 2 & \frac83 & 0 \\[3pt]
      -1 & \frac12 & -\frac13 & 1
    \end{bmatrix}.
  \end{equation*}
  We can combine these into one augmented matrix and perform
  row-reduction, which gives
  \begin{equation*}
    \begin{bmatrix}
      1 & \frac12 & \frac13 & 1 & 0 & 0 \\[3pt]
      2 & 2 & \frac83 & 0 & 1 & 0 \\[3pt]
      -1 & \frac12 & -\frac13 & 0 & 0 & 1
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
      1 & 0 & 0 & 1 & -\frac16 & -\frac13 \\[3pt]
      0 & 1 & 0 & 1 & 0 & 1 \\[3pt]
      0 & 0 & 1 & -\frac32 & \frac12 & -\frac12
    \end{bmatrix}.
  \end{equation*}
  So we see that
  \begin{align*}
    p_1(x) &= 1 + x - \frac32x^2, \\
    p_2(x) &= -\frac16 + \frac12x^2, \\
    p_3(x) &= -\frac13 + x - \frac12x^2,
  \end{align*}
  and $\{f_1,f_2,f_3\}$ is the dual basis of $\{p_1,p_2,p_3\}$.
\end{solution}

\Exercise5 If $A$ and $B$ are $n\times n$ complex matrices, show that
$AB - BA = I$ is impossible.
\begin{proof}
  In Example~19, the trace function was shown to be a linear
  functional on the space of $n\times n$ matrices. And in
  Exercise~\ref{exercise:lin-tran:trace-AB-eq-trace-BA}, we proved
  that, given two matrices $A$ and $B$, $\tr(AB) = \tr(BA)$. It now
  follows that
  \begin{equation*}
    \tr(AB - BA) = \tr(AB) - \tr(BA) = 0.
  \end{equation*}
  But $\tr(I) = n\neq0$, so it cannot be the case that $AB - BA = I$.
\end{proof}

\Exercise6 Let $m$ and $n$ be positive integers and $F$ a field. Let
$f_1,\dots,f_m$ be linear functionals on $F^n$. For $\alpha$ in $F^n$
define
\begin{equation*}
  T\alpha = (f_1(\alpha), \dots, f_m(\alpha)).
\end{equation*}
Show that $T$ is a linear transformation from $F^n$ into $F^m$. Then
show that every linear transformation from $F^n$ into $F^m$ is of the
above form, for some $f_1,\dots,f_m$.
\begin{proof}
  First, for any $\alpha_1,\alpha_2$ in $F^n$ and any $c$ in $F$, we
  have
  \begin{align*}
    T(c\alpha_1 + \alpha_2)
    &= (f_1(c\alpha_1 + \alpha_2), \dots, f_m(c\alpha_1 + \alpha_2)) \\
    &= (cf_1(\alpha_1) + f_1(\alpha_2),
      \dots, cf_m(\alpha_1) + f_m(\alpha_2)) \\
    &= c(f_1(\alpha_1), \dots, f_m(\alpha_1))
      + (f_1(\alpha_2), \dots, f_m(\alpha_2)) \\
    &= cT\alpha_1 + T\alpha_2.
  \end{align*}
  This shows that $T$ is a linear transformation from $F^n$ into
  $F^m$.

  Next, let $T$ be any linear transformation from $F^n$ into
  $F^m$. For each $i$ with $1\leq i\leq m$, define $f_i(\alpha)$ to be
  the $i$th coordinate of $T\alpha$. Then
  \begin{equation*}
    T\alpha = (f_1(\alpha), \dots, f_m(\alpha))
  \end{equation*}
  and, moreover, each $f_i$ is a linear transformation because $T$
  itself is linear. Therefore every linear transformation from $F^n$
  into $F^m$ can be written this way.
\end{proof}

\Exercise7 Let $\alpha_1 = (1, 0, -1, 2)$ and
$\alpha_2 = (2, 3, 1, 1)$, and let $W$ be the subspace of $R^4$
spanned by $\alpha_1$ and $\alpha_2$. Which linear functionals $f$:
\begin{equation*}
  f(x_1,x_2,x_3,x_4) = c_1x_1 + c_2x_2 + c_3x_3 + c_4x_4
\end{equation*}
are in the annihilator of $W$?
\begin{solution}
  Let $f$ be in $W^0$. We want
  \begin{equation*}
    f(1, 0, -1, 2) = f(2, 3, 1, 1) = 0.
  \end{equation*}
  This leads to a system of equations in $c_1,c_2,c_3,c_4$ having
  coefficient matrix
  \begin{equation*}
    \begin{bmatrix}
      1 & 0 & -1 & 2 \\
      2 & 3 & 1 & 1
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
      1 & 0 & -1 & 2 \\
      0 & 1 & 1 & -1
    \end{bmatrix}.
  \end{equation*}
  From the reduced form we see that $c_3$ and $c_4$ can be arbitrary,
  with
  \begin{equation*}
    c_1 = c_3 - 2c_4 \quad\text{and}\quad c_2 = c_4 - c_3.
  \end{equation*}
  Therefore $W^0$ consists of the linear functionals $f$ having the
  form
  \begin{equation*}
    f(x_1,x_2,x_3,x_4) = (s - 2t)x_1 + (t - s)x_2 + sx_3 + tx_4,
  \end{equation*}
  where $s$ and $t$ are scalars in $F$. Note that we can also find a
  basis for $W^0$ by first taking $s = 1, t = 0$ and then by taking
  $s = 0, t = 1$.
\end{solution}

\Exercise8 Let $W$ be the subspace of $R^5$ which is spanned by the
vectors
\begin{align*}
  \alpha_1
  &= \epsilon_1 + 2\epsilon_2 + \epsilon_3, \quad
    \alpha_2 = \epsilon_2 + 3\epsilon_3 + 3\epsilon_4 + \epsilon_5 \\
  \alpha_3
  &= \epsilon_1 + 4\epsilon_2 + 6\epsilon_3 + 4\epsilon_4 + \epsilon_5.
\end{align*}
Find a basis for $W^0$.
\begin{solution}
  Take $f$ in $W^0$, and write
  \begin{equation*}
    f(x_1,x_2,x_3,x_4,x_5)
    = c_1x_1 + c_2x_2 + c_3x_3 + c_4x_4 + c_5x_5.
  \end{equation*}
  Since $f$ annihilates $W$, we have
  \begin{alignat*}{7}
    f(\alpha_1) &{}={}&
    c_1 &{}+{}& 2c_2 &{}+{}& c_3 && && &{}={}& 0 & \\
    f(\alpha_2) &{}={}&
    && c_2 &{}+{}& 3c_3 &{}+{}& 3c_4 &{}+{}& c_5 &{}={}& 0 & \\
    f(\alpha_3) &{}={}&
    c_1 &{}+{}& 4c_2 &{}+{}& 6c_3 &{}+{}& 4c_4 &{}+{}& c_5
    &{}={}& 0 &.
  \end{alignat*}
  The coefficient matrix for this system reduces as follows:
  \begin{equation*}
    \begin{bmatrix}
      1 & 2 & 1 & 0 & 0 \\
      0 & 1 & 3 & 3 & 1 \\
      1 & 4 & 6 & 4 & 1
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
      1 & 0 & 0 & 4 & 3 \\
      0 & 1 & 0 & -3 & -2 \\
      0 & 0 & 1 & 2 & 1
    \end{bmatrix}.
  \end{equation*}
  Since the latter matrix has three nonzero rows, we see that $W$ has
  dimension $3$ and $\{\alpha_1,\alpha_2,\alpha_3\}$ is a basis for
  $W$. We also see that
  \begin{align*}
    c_1 &= -4c_4 - 3c_5, \\
    c_2 &= 3c_4 + 2c_5, \\
    \intertext{and}
    c_3 &= -2c_4 - c_5.
  \end{align*}
  Therefore the set $\{f_1,f_2\}$, where
  \begin{equation*}
    f_1(x_1,x_2,x_3,x_4,x_5)
    = -4x_1 + 3x_2 - 2x_3 + x_4
  \end{equation*}
  and
  \begin{equation*}
    f_2(x_1,x_2,x_3,x_4,x_5)
    = -3x_1 + 2x_2 - x_3 + x_5,
  \end{equation*}
  is a basis for $W^0$. Note that $\dim W^0 = 2$, which agrees with
  Theorem~16.
\end{solution}

\Exercise9 Let $V$ be the vector space of all $2\times2$ matrices over
the field of real numbers, and let
\begin{equation*}
  B =
  \begin{bmatrix}
    2 & -2 \\
    -1 & 1
  \end{bmatrix}.
\end{equation*}
Let $W$ be the subspace of $V$ consisting of all $A$ such that
$AB = 0$. Let $f$ be a linear functional on $V$ which is in the
annihilator of $W$. Suppose that $f(I) = 0$ and $f(C) = 3$, where $I$
is the $2\times2$ identity matrix and
\begin{equation*}
  C =
  \begin{bmatrix}
    0 & 0 \\
    0 & 1
  \end{bmatrix}.
\end{equation*}
Find $f(B)$.
\begin{solution}
  Note that
  \begin{equation*}
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      2 & -2 \\
      -1 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
      2a - b & -2a + b \\
      2c - d & -2c + d
    \end{bmatrix}
    = 0
  \end{equation*}
  if and only if $2a = b$ and $2c = d$. Therefore, a basis for $W$ is
  \begin{equation*}
    \left\{
      \begin{bmatrix}
        1 & 2 \\ 0 & 0
      \end{bmatrix},
      \begin{bmatrix}
        0 & 0 \\ 1 & 2
      \end{bmatrix}
    \right\}.
  \end{equation*}

  Now, let
  \begin{equation*}
    f(A) = c_1A_{11} + c_2A_{12} + c_3A_{21} + c_4A_{22},
  \end{equation*}
  where $A$ is any $2\times2$ matrix. We know that $f$ annihilates the
  two basis vectors for $W$ found above, and we also know that
  $f(I) = 0$ and $f(C) = 3$. This leads to the following system of
  linear equations:
  \begin{align*}
    c_1 + 2c_2 &= 0 \\
    c_3 + 2c_4 &= 0 \\
    c_1 + c_4 &= 0 \\
    c_4 &= 3.
  \end{align*}
  This system has the unique solution
  \begin{equation*}
    c_1 = -3, \quad
    c_2 = \frac32, \quad
    c_3 = -6, \quad
    c_4 = 3,
  \end{equation*}
  so
  \begin{equation*}
    f(A) = -3A_{11} + \frac32A_{12} - 6A_{21} + 3A_{22}.
  \end{equation*}
  Therefore
  \begin{equation*}
    f(B) = -3(2) + \frac32(-2) - 6(-1) + 3(1) = 0. \qedhere
  \end{equation*}
\end{solution}

\Exercise{10} Let $F$ be a subfield of the complex numbers. We define
$n$ linear functionals on $F^n$ ($n\geq2$) by
\begin{equation*}
  f_k(x_1,\dots,x_n) = \sum_{j=1}^n(k-j)x_j, \quad 1\leq k\leq n.
\end{equation*}
What is the dimension of the subspace annihilated by $f_1,\dots,f_n$?
\begin{solution}
  Call the subspace $W$. We first want to find $\dim W^0$. Fix some
  $n\geq2$ and consider the set $\mathcal{B} = \{f_1,f_2\}$. Since
  $f_1(\alpha) = f_2(\alpha) = 0$ for any $\alpha$ in $W$, we have
  \begin{align*}
    -x_2 - 2x_3 - 3x_4 - \cdots - (n - 1)x_n &= 0, \\
    x_1 - x_3 - 2x_4 - \cdots - (n - 2)x_n &= 0.
  \end{align*}
  The coefficient matrix for the above system of equations is given by
  \begin{equation*}
    A =
    \begin{bmatrix}
      0 & -1 & -2 & -3 & \cdots & 1 - n \\
      1 & 0 & -1 & -2 & \cdots & 2 - n
    \end{bmatrix}.
  \end{equation*}
  Observe that by multiplying the first row by $-1$ and interchanging
  the two rows, we can put $A$ in row-reduced echelon form. Since $A$
  is row-equivalent to a row-reduced matrix having two nonzero rows,
  we see that the set $\mathcal{B}$ is linearly independent.

  Next, consider $f_k$ for some $k$ with $3\leq k\leq n$. Then we can
  write
  \begin{align*}
    f_k(x_1,\dots,x_n)
    &= \sum_{j=1}^n(k-j)x_j \\
    &= \sum_{j=1}^n(-k - 2j + 2 + jk)x_j
      + \sum_{j=1}^n(2k + j - 2 - jk)x_j \\
    &= (2 - k)\sum_{j=1}^n(1 - j)x_j + (k - 1)\sum_{j=1}^n(2 - j)x_j \\
    &= (2 - k)f_1(x_1,\dots,x_n) + (k - 1)f_2(x_1,\dots,x_n).
  \end{align*}
  This shows that $\mathcal{B}$ spans the annihilator of
  $W$. Therefore $\mathcal{B}$ is a basis for $W^0$ and
  $\dim W^0 = 2$. It now follows from Theorem~16 that
  $\dim W = n - 2$.
\end{solution}

\Exercise{11} Let $W_1$ and $W_2$ be subspaces of a finite-dimensional
vector space $V$.
\begin{enumerate}
\item Prove that $(W_1+W_2)^0 = W_1^0\cap W_2^0$.
  \begin{proof}
    If a linear functional $f$ belongs to $(W_1 + W_2)^0$, then it
    annihilates every vector in $W_1 + W_2$. But $W_1$ and $W_2$ are
    subspaces of $W_1 + W_2$, so $f$ must belong to $W_1^0\cap W_2^0$.

    Conversely, if $f$ belongs to $W_1^0\cap W_2^0$, then $f$
    annihilates all vectors in $W_1$, and also annihilates all vectors
    in $W_2$. Since $f$ is linear, it must therefore annihilate sums
    of these vectors, so $f$ is in $(W_1 + W_2)^0$.

    We have shown that members of $(W_1 + W_2)^0$ are members of
    $W_1^0\cap W_2^0$ and vice versa, so these spaces are equal.
  \end{proof}

\item Prove that $(W_1\cap W_2)^0 = W_1^0 + W_2^0$.
  \begin{proof}
    Let $\mathcal{B} = \{\alpha_1, \dots, \alpha_m\}$ be a basis for
    $W_1\cap W_2$. Extend this basis to a basis $\mathcal{B}'$ for
    $W_1$, where
    \begin{equation*}
      \mathcal{B}' = \{\alpha_1, \dots, \alpha_m,
      \beta_1, \dots, \beta_n\}.
    \end{equation*}
    Also extend $\mathcal{B}$ to a basis $\mathcal{B}''$ for $W_2$,
    where
    \begin{equation*}
      \mathcal{B}'' = \{\alpha_1, \dots, \alpha_m,
      \gamma_1, \dots, \gamma_p\}.
    \end{equation*}

    Now take any linear functional $f$ belonging to $(W_1\cap
    W_2)^0$. Let $g$ be the linear functional on $V$ such that
    \begin{gather*}
      g(\alpha_1) = \cdots = g(\alpha_m) = 0, \\
      g(\beta_1) = \cdots = g(\beta_n) = 0, \\
      g(\gamma_1) = f(\gamma_1), \quad\dots,\quad
      g(\gamma_p) = f(\gamma_p).
    \end{gather*}
    Notice that $g$ belongs to $W_1^0$. Similarly, define $h$ to be
    the linear functional given by
    \begin{gather*}
      h(\alpha_1) = \cdots = h(\alpha_m) = 0, \\
      h(\gamma_1) = \cdots = h(\gamma_p) = 0, \\
      h(\beta_1) = f(\beta_1), \quad\dots,\quad
      h(\beta_n) = f(\beta_n).
    \end{gather*}
    Notice that $h$ belongs to $W_2^0$. Moreover, $f = g + h$. This
    shows that $f$ belongs to the sum $W_1^0 + W_2^0$, and we see that
    $(W_1\cap W_2)^0$ is a subset of $W_1^0 + W_2^0$.

    Next, suppose $f = f_1 + f_2$, where $f_1$ is in $W_1^0$ and $f_2$
    is in $W_2^0$. Since $W_1\cap W_2$ is a subspace of $W_1$ and also
    a subspace of $W_2$, it follows that both $f_1$ and $f_2$
    annihilate $W_1\cap W_2$, i.e. $f$ belongs to $(W_1\cap
    W_2)^0$. This completes the proof that
    $(W_1\cap W_2)^0 = W_1^0 + W_2^0$.
  \end{proof}
\end{enumerate}

\Exercise{12} Let $V$ be a finite-dimensional vector space over the
field $F$ and let $W$ be a subspace of $V$. If $f$ is a linear
functional on $W$, prove that there is a linear functional $g$ on $V$
such that $g(\alpha) = f(\alpha)$ for each $\alpha$ in the subspace
$W$.
\begin{proof}
  Let $\mathcal{B} = \{\alpha_1,\dots,\alpha_m\}$ be a basis for $W$
  and extend this basis to a basis
  \begin{equation*}
    \mathcal{B}' = \{\alpha_1,\dots,\alpha_m,\beta_1,\dots,\beta_n\}
  \end{equation*}
  for $V$.

  Let $f$ be any linear functional on $W$. Define the linear
  functional $g$ on $V$ by
  \begin{equation*}
    g(\alpha_i) = f(\alpha_i),
    \quad \text{for $1\leq i\leq m$},
    \quad\text{and}\quad
    g(\beta_j) = 0,
    \quad \text{for $1\leq j\leq n$}.
  \end{equation*}
  We know $g$ exists by Theorem~1. So $g$ is a linear functional on
  $V$ that agrees with $f$ on $W$, as we wanted to show.
\end{proof}

\Exercise{13} Let $F$ be a subfield of the field of complex numbers
and let $V$ be any vector space over $F$. Suppose that $f$ and $g$ are
linear functionals on $V$ such that the function $h$ defined by
$h(\alpha) = f(\alpha)g(\alpha)$ is also a linear functional on
$V$. Prove that either $f = 0$ or $g = 0$.
\begin{proof}
  Let $f$, $g$, and $h$ be the linear functionals on $V$ with the
  properties described above.

  Choose any $\alpha$, $\beta$ in $V$. Then
  \begin{align*}
    h(\alpha + \beta)
    &= f(\alpha + \beta)g(\alpha + \beta) \\
    &= (f(\alpha) + f(\beta))(g(\alpha) + g(\beta)) \\
    &= f(\alpha)g(\alpha) + f(\alpha)g(\beta) + f(\beta)g(\alpha)
      + f(\beta)g(\beta) \\
    &= h(\alpha) + h(\beta) + f(\alpha)g(\beta) + f(\beta)g(\alpha) \\
    &= h(\alpha + \beta) + f(\alpha)g(\beta) + f(\beta)g(\alpha).
  \end{align*}
  This shows that for any pair of vectors $\alpha$, $\beta$ in $V$,
  \begin{equation}
    \label{eq:lin-tran:fa-gb-plus-fb-ga-eq-0}
    f(\alpha)g(\beta) + f(\beta)g(\alpha) = 0.
  \end{equation}
  Taking $\beta = \alpha$, we also see that $h = 0$.

  Now, either $f = 0$ or $f\neq0$. If $f = 0$ then there is nothing
  left to prove, so we will assume that $f\neq0$. Again, let $\alpha$
  in $V$ be arbitrary, and let $\beta$ in $V$ be such that $f(\beta)$
  is nonzero. We know that
  \begin{equation*}
    f(\beta)g(\beta) = 0,
  \end{equation*}
  and it follows that $g(\beta) = 0$ (since
  $f(\beta)\neq0$). Substituting zero for $g(\beta)$ in equation
  \eqref{eq:lin-tran:fa-gb-plus-fb-ga-eq-0} then gives
  \begin{equation*}
    f(\beta)g(\alpha) = 0.
  \end{equation*}
  But, again, $f(\beta)$ is nonzero, so we must have $g(\alpha) =
  0$. Since $\alpha$ was chosen arbitrarily, it follows that $g = 0$
  and the proof is complete.
\end{proof}

\Exercise{14} Let $F$ be a field of characteristic zero and let $V$ be
a finite-dimensional vector space over $F$. If
$\alpha_1,\dots,\alpha_m$ are finitely many vectors in $V$, each
different from the zero vector, prove that there is a linear
functional $f$ on $V$ such that
\begin{equation*}
  f(\alpha_i) \neq 0, \quad i = 1, \dots, m.
\end{equation*}
\begin{proof}
  Let $\mathcal{B} = \{\beta_1,\dots,\beta_n\}$ be a basis for $V$. We
  will use induction on $m$ to show that we can always find a linear
  functional $f$ such that $f(\alpha_i)\neq0$ for all $i$.

  First, when $m = 1$, we have a single nonzero vector
  $\alpha_1$. Writing $\alpha_1$ as a linear combination of the
  vectors in $\mathcal{B}$ gives
  \begin{equation*}
    \alpha_1 = c_1\beta_1 + c_2\beta_2 + \cdots + c_n\beta_n,
  \end{equation*}
  for some scalars $c_1,\dots,c_n$ in $F$. Since $\alpha_1\neq0$,
  there is an index $k$ such that $c_k\neq0$. Now define $f$ to be the
  linear functional on $V$ such that
  \begin{equation*}
    f(\beta_i) = \delta_{ik}c_k^{-1},
  \end{equation*}
  where $\delta_{ik}$ is the Kronecker delta. We now have
  \begin{equation*}
    f(\alpha_1) = c_kc_k^{-1} = 1 \neq 0.
  \end{equation*}
  This shows that the statement holds for the base case of $m = 1$.

  Now assume that the statement is true when $m = k$ for some
  $k\geq1$, and let $k + 1$ nonzero vectors
  $\alpha_1,\dots,\alpha_{k+1}$ be given. We may apply the inductive
  hypothesis to find a linear functional $f_0$ on $V$ such that
  $f_0(\alpha_i) \neq 0$ for all $i$ with $1\leq i\leq k$.

  If $f_0(\alpha_{k+1})$ happens to be nonzero, then we are done. So
  we will assume that $f_0(\alpha_{k+1}) = 0$. Again the inductive
  hypothesis, when applied to the single vector $\alpha_{k+1}$, allows
  us to find a linear functional $f_1$ such that
  $f_1(\alpha_{k+1})\neq0$.

  For each $i$ with $1\leq i\leq k+1$, define the number $N_i$ as
  follows. First, if there is no positive integer $n$ such that
  \begin{equation}
    \label{eq:lin-tran:f0a-plus-n-f1a-eq-0}
    f_0(\alpha_i) + nf_1(\alpha_i) = 0,
  \end{equation}
  then set $N_i = 1$. Otherwise, define $N_i$ to be the unique
  positive integer such that
  \begin{equation*}
    f_0(\alpha_i) + N_if_1(\alpha_i) = 0.
  \end{equation*}
  In this second case, we know that $N_i$ is unique for the following
  reason. Suppose $n = M$ and $n = N_i$ both satisfy
  \eqref{eq:lin-tran:f0a-plus-n-f1a-eq-0}. By construction, it is not
  possible for $f_0(\alpha_i)$ and $f_1(\alpha_i)$ to both be
  zero. But if one is zero, then
  \eqref{eq:lin-tran:f0a-plus-n-f1a-eq-0} implies that the other is as
  well (because $F$ has characteristic zero). So we see that neither
  $f_0(\alpha_i)$ nor $f_1(\alpha_i)$ is zero, hence
  \begin{equation*}
    M = -\frac{f_0(\alpha_i)}{f_1(\alpha_i)} = N_i.
  \end{equation*}
  So $N_i$ is well-defined for all $i = 1, \dots, k+1$.

  Now, the set
  \begin{equation*}
    A = \{N_1, N_2, \dots, N_{k+1}\}
  \end{equation*}
  is a finite set of natural numbers, so we can find a largest element
  in $A$. Take any natural number $P$ greater than this largest
  element, and define the linear functional $f$ on $V$ by
  \begin{equation*}
    f = f_0 + Pf_1.
  \end{equation*}
  Then we see that $f(\alpha_i)\neq0$ for each $i = 1,\dots,k+1$,
  completing the inductive step of the proof.

  By induction, the original statement must be true for all positive
  integers $m$.
\end{proof}

\Exercise{15} According to
Exercise~\ref{exercise:lin-tran:trace-AB-eq-trace-BA}, similar
matrices have the same trace. Thus we can define the trace of a linear
operator on a finite-dimensional space to be the trace of any matrix
which represents the operator in an ordered basis. This is
well-defined since all such representing matrices for one operator are
similar.

Now let $V$ be the space of all $2\times2$ matrices over the field $F$
and let $P$ be a fixed $2\times2$ matrix. Let $T$ be the linear
operator on $V$ defined by $T(A) = PA$. Prove that
$\trace(T) = 2\trace(P)$.
\begin{proof}
  Let
  \begin{equation*}
    P =
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
  \end{equation*}
  and let
  \begin{equation*}
    \mathcal{B} = \left\{
      \begin{bmatrix}
        1 & 0 \\
        0 & 0
      \end{bmatrix},
      \begin{bmatrix}
        0 & 1 \\
        0 & 0
      \end{bmatrix},
      \begin{bmatrix}
        0 & 0 \\
        1 & 0
      \end{bmatrix},
      \begin{bmatrix}
        0 & 0 \\
        0 & 1
      \end{bmatrix}
    \right\}
  \end{equation*}
  be an ordered basis for $V$. We calculate
  \begin{align*}
    T
    \begin{bmatrix}
      1 & 0 \\
      0 & 0
    \end{bmatrix}
    &=
    \begin{bmatrix}
      a & 0 \\
      c & 0
    \end{bmatrix}
    = a
    \begin{bmatrix}
      1 & 0 \\
      0 & 0
    \end{bmatrix}
    + c
    \begin{bmatrix}
      0 & 0 \\
      1 & 0
    \end{bmatrix}, \\
    T
    \begin{bmatrix}
      0 & 1 \\
      0 & 0
    \end{bmatrix}
    &=
    \begin{bmatrix}
      0 & a \\
      0 & c
    \end{bmatrix}
    = a
    \begin{bmatrix}
      0 & 1 \\
      0 & 0
    \end{bmatrix}
    + c
    \begin{bmatrix}
      0 & 0 \\
      0 & 1
    \end{bmatrix}, \\
    T
    \begin{bmatrix}
      0 & 0 \\
      1 & 0
    \end{bmatrix}
    &=
    \begin{bmatrix}
      b & 0 \\
      d & 0
    \end{bmatrix}
    = b
    \begin{bmatrix}
      1 & 0 \\
      0 & 0
    \end{bmatrix}
    + d
    \begin{bmatrix}
      0 & 0 \\
      1 & 0
    \end{bmatrix}, \\
    T
    \begin{bmatrix}
      0 & 0 \\
      0 & 1
    \end{bmatrix}
    &=
    \begin{bmatrix}
      0 & b \\
      0 & d
    \end{bmatrix}
    = b
    \begin{bmatrix}
      0 & 1 \\
      0 & 0
    \end{bmatrix}
    + d
    \begin{bmatrix}
      0 & 0 \\
      0 & 1
    \end{bmatrix}.
  \end{align*}
  From this, we see that the matrix for $T$ relative to $\mathcal{B}$
  is the $4\times4$ matrix
  \begin{equation*}
    [T]_{\mathcal{B}} =
    \begin{bmatrix}
      a & 0 & b & 0 \\
      0 & a & 0 & b \\
      c & 0 & d & 0 \\
      0 & c & 0 & d
    \end{bmatrix}.
  \end{equation*}
  We can now readily see that
  \begin{equation*}
    \trace(T) = \tr([T]_{\mathcal{B}}) = 2a + 2d = 2\tr(P). \qedhere
  \end{equation*}
\end{proof}

\Exercise{16} Show that the trace functional on $n\times n$ matrices
is unique in the following sense. If $W$ is the space of $n\times n$
matrices over the field $F$ and if $f$ is a linear functional on $W$
such that $f(AB) = f(BA)$ for each $A$ and $B$ in $W$, then $f$ is a
scalar multiple of the trace function. If, in addition, $f(I) = n$,
then $f$ is the trace function.
\begin{proof}
  Let
  \begin{equation*}
    \mathcal{B} = \{\epsilon_{11}, \epsilon_{12}, \dots, \epsilon_{1n},
    \dots, \epsilon_{n1}, \epsilon_{n2}, \dots, \epsilon_{nn}\}
  \end{equation*}
  be the basis for $W$ where $\epsilon_{ij}$ is the $n\times n$ matrix
  having a $1$ in the $i,j$th entry and all other entries $0$. Since
  $f$ is linear, we may write it as
  \begin{equation}
    \label{eq:lin-tran:f-mult-of-trace-general}
    f(A) = \sum_{i=1}^n\sum_{j=1}^nC_{ij}A_{ij},
  \end{equation}
  where each $C_{ij}$ is a fixed constant and $A_{ij}$ is the $i,j$th
  entry of $A$.

  Note that the $p,q$th entry of $\epsilon_{ij}$ is
  $\delta_{ip}\delta_{qj}$, where $\delta_{ij}$ is the Kronecker
  delta. So, if we fix some indices $i,j,a,b$, each in the range from
  $1$ to $n$, then we have
  \begin{equation}
    \label{eq:lin-tran:f-on-prod-basis-vecs}
    f(\epsilon_{ij}\epsilon_{ab})
    = \sum_{p=1}^n\sum_{q=1}^nC_{pq}\sum_{k=1}^n(\delta_{ip}\delta_{kj})
      (\delta_{ak}\delta_{qb})
    = C_{ib}\delta_{aj}.
  \end{equation}
  From this, we see that
  \begin{equation*}
    f(\epsilon_{ij}\epsilon_{ji}) = C_{ii} \quad\text{and}\quad
    f(\epsilon_{ji}\epsilon_{ij}) = C_{jj}.
  \end{equation*}
  Since these must be equal, we see that
  $C_{11} = C_{22} = \cdots = C_{nn}$. On the other hand,
  \eqref{eq:lin-tran:f-on-prod-basis-vecs} also gives
  \begin{equation*}
    f(\epsilon_{i1}\epsilon_{1j}) = C_{ij} \quad\text{and}\quad
    f(\epsilon_{1j}\epsilon_{i1}) = C_{11}\delta_{ij}.
  \end{equation*}
  These must be equal, so by looking at values of $i$ and $j$ where
  $i\neq j$, we see that $C_{ij} = 0$ whenever $i\neq j$. Therefore,
  equation~\eqref{eq:lin-tran:f-mult-of-trace-general} can be
  simplified to
  \begin{equation*}
    f(A) = \sum_{k=1}^nCA_{kk}
    = C(A_{11} + A_{22} + \cdots + A_{nn}) = C\tr(A),
  \end{equation*}
  where $C = C_{11}$ is a constant. We conclude that $f$ is a scalar
  multiple of the trace functional. If we require $f(I) = n$, then we
  must have $C = 1$ and $f = \tr$.
\end{proof}
