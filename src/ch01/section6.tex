\section{Invertible Matrices}

\Exercise1
\label{eq:lin-eq:inv:R-eq-P-A}
Let
\begin{equation*}
  A =
  \begin{bmatrix}
    1 & 2 & 1 & 0 \\
    -1 & 0 & 3 & 5 \\
    1 & -2 & 1 & 1
  \end{bmatrix}.
\end{equation*}
Find a row-reduced echelon matrix $R$ which is row-equivalent to $A$
and an invertible $3\times3$ matrix $P$ such that $R = PA$.
\begin{solution}
  We can perform elementary row operations on $A$, while performing
  the same operations on $I$, in order to find $R$ and $P$:
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 1 & 0 \\
      -1 & 0 & 3 & 5 \\
      1 & -2 & 1 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 2 & 1 & 0 \\
      0 & 2 & 4 & 5 \\
      0 & -4 & 0 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 0 \\
      1 & 1 & 0 \\
      -1 & 0 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & -3 & -5 \\
      0 & 2 & 4 & 5 \\
      0 & 0 & 8 & 11
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      0 & -1 & 0 \\
      1 & 1 & 0 \\
      1 & 2 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & -3 & -5 \\[3pt]
      0 & 1 & 2 & \frac52 \\[3pt]
      0 & 0 & 1 & \frac{11}8
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      0 & -1 & 0 \\[3pt]
      \frac12 & \frac12 & 0 \\[3pt]
      \frac18 & \frac14 & \frac18
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & 0 & -\frac78 \\[3pt]
      0 & 1 & 0 & -\frac14 \\[3pt]
      0 & 0 & 1 & \frac{11}8
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      \frac38 & -\frac14 & \frac38 \\[3pt]
      \frac14 & 0 & -\frac14 \\[3pt]
      \frac18 & \frac14 & \frac18
    \end{bmatrix}.
  \end{align*}
  Therefore,
  \begin{equation*}
    R =
    \begin{bmatrix}
      1 & 0 & 0 & -\frac78 \\[3pt]
      0 & 1 & 0 & -\frac14 \\[3pt]
      0 & 0 & 1 & \frac{11}8
    \end{bmatrix},
    \quad
    P =
    \begin{bmatrix}
      \frac38 & -\frac14 & \frac38 \\[3pt]
      \frac14 & 0 & -\frac14 \\[3pt]
      \frac18 & \frac14 & \frac18
    \end{bmatrix}
    =
    \frac18
    \begin{bmatrix}
      3 & -2 & 3 \\
      2 & 0 & -2 \\
      1 & 2 & 1
    \end{bmatrix},
  \end{equation*}
  and $R = PA$.
\end{solution}

\Exercise2 Do Exercise~\ref{eq:lin-eq:inv:R-eq-P-A}, but with
\begin{equation*}
  A =
  \begin{bmatrix}
    2 & 0 & i \\
    1 & -3 & -i \\
    i & 1 & 1
  \end{bmatrix}.
\end{equation*}
\begin{solution}
  We proceed as before:
  \begin{align*}
    \begin{bmatrix}
      2 & 0 & i \\
      1 & -3 & -i \\
      i & 1 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & -3 & -i \\
      2 & 0 & i \\
      i & 1 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      0 & 0 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & -3 & -i \\
      0 & 6 & 3i \\
      0 & 1 + 3i & 0
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      0 & 1 & 0 \\
      1 & -2 & 0 \\
      0 & -i & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & -3 & -i \\[3pt]
      0 & 1 & \frac12i \\[3pt]
      0 & 1 + 3i & 0
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      0 & 1 & 0 \\[3pt]
      \frac16 & -\frac13 & 0 \\[3pt]
      0 & -i & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & \frac12i \\[3pt]
      0 & 1 & \frac12i \\[3pt]
      0 & 0 & \frac32 - \frac12i
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      \frac12 & 0 & 0 \\[3pt]
      \frac16 & -\frac13 & 0 \\[3pt]
      -\frac16 - \frac12i & \frac13 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & \frac12i \\[3pt]
      0 & 1 & \frac12i \\[3pt]
      0 & 0 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      \frac12 & 0 & 0 \\[3pt]
      \frac16 & -\frac13 & 0 \\[3pt]
      -\frac13i & \frac15 + \frac1{15}i & \frac35 + \frac15i
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & 0 \\[3pt]
      0 & 1 & 0 \\[3pt]
      0 & 0 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      \frac13 & \frac1{30}-\frac1{10}i & \frac1{10}-\frac3{10}i \\[3pt]
      0 & -\frac3{10}-\frac1{10}i & \frac1{10} - \frac3{10}i \\[3pt]
      -\frac13i & \frac15 + \frac1{15}i & \frac35 + \frac15i
    \end{bmatrix}.
  \end{align*}
  So
  \begin{equation*}
    R =
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    = I,
    \quad
    P = \frac1{30}
    \begin{bmatrix}
      10 & 1 - 3i & 3 - 9i \\
      0 & -9 - 3i & 3 - 9i \\
      -10i & 6 + 2i & 18 + 6i
    \end{bmatrix},
  \end{equation*}
  and $R = PA$.
\end{solution}

\Exercise3 For each of the two matrices
\begin{equation*}
  \begin{bmatrix}
    2 & 5 & -1 \\
    4 & -1 & 2 \\
    6 & 4 & 1
  \end{bmatrix},
  \quad
  \begin{bmatrix}
    1 & -1 & 2 \\
    3 & 2 & 4 \\
    0 & 1 & -2
  \end{bmatrix}
\end{equation*}
use elementary row operations to discover whether it is invertible,
and to find the inverse in case it is.
\begin{solution}
  For the first matrix, row-reduction gives
  \begin{gather*}
    \begin{bmatrix}
      2 & 5 & -1 \\
      4 & -1 & 2 \\
      6 & 4 & 1
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      1 & \frac52 & -\frac12 \\[3pt]
      4 & -1 & 2 \\[3pt]
      6 & 4 & 1
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      1 & \frac52 & -\frac12 \\[3pt]
      0 & -11 & 4 \\[3pt]
      0 & -11 & 4
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      1 & \frac52 & -\frac12 \\[3pt]
      0 & -11 & 4 \\[3pt]
      0 & 0 & 0
    \end{bmatrix},
  \end{gather*}
  and we see that the original matrix is not invertible since it is
  row-equivalent to a matrix having a row of zeros.

  For the second matrix, we get
  \begin{align*}
    \begin{bmatrix}
      1 & -1 & 2 \\
      3 & 2 & 4 \\
      0 & 1 & -2
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & -1 & 2 \\
      0 & 5 & -2 \\
      0 & 1 & -2
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 0 \\
      -3 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & -1 & 2 \\
      0 & 1 & -2 \\
      0 & 5 & -2
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 0 & 1 \\
      -3 & 1 & 0
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & -2 \\
      0 & 0 & 8
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 1 \\
      0 & 0 & 1 \\
      -3 & 1 & -5
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & 0 \\[3pt]
      0 & 1 & -2 \\[3pt]
      0 & 0 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 1 \\[3pt]
      0 & 0 & 1 \\[3pt]
      -\frac38 & \frac18 & -\frac58
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & 0 \\[3pt]
      0 & 1 & 0 \\[3pt]
      0 & 0 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 1 \\[3pt]
      -\frac34 & \frac14 & -\frac14 \\[3pt]
      -\frac38 & \frac18 & -\frac58
    \end{bmatrix}.
  \end{align*}
  From this we see that the original matrix is invertible and its
  inverse is the matrix
  \begin{equation*}
    \frac18
    \begin{bmatrix}
      8 & 0 & 8 \\
      -6 & 2 & -2 \\
      -3 & 1 & -5
    \end{bmatrix}. \qedhere
  \end{equation*}
\end{solution}

\Exercise4 Let
\begin{equation*}
  A =
  \begin{bmatrix}
    5 & 0 & 0 \\
    1 & 5 & 0 \\
    0 & 1 & 5
  \end{bmatrix}.
\end{equation*}
For which $X$ does there exist a scalar $c$ such that $AX = cX$?
\begin{solution}
  Let
  \begin{equation*}
    X =
    \begin{bmatrix}
      x_1 \\
      x_2 \\
      x_3
    \end{bmatrix}.
  \end{equation*}
  Then $AX = cX$ implies
  \begin{align*}
    5x_1 &= cx_1 \\
    x_1 + 5x_2 &= cx_2 \\
    x_2 + 5x_3 &= cx_3,
  \end{align*}
  and this is a homogeneous system of equations with coefficient
  matrix
  \begin{equation*}
    B =
    \begin{bmatrix}
      5 - c & 0 & 0 \\
      1 & 5 - c & 0 \\
      0 & 1 & 5 - c
    \end{bmatrix}.
  \end{equation*}
  If $c = 5$ then $(x_1,x_2,x_3) = (0,0,t)$ for some scalar $t$, so
  this gives one possibility for $X$. If we assume $c\neq5$, then the
  matrix $B$ can be row-reduced to the identity matrix, so that
  $X = 0$ is then the only possibility. Therefore, there is a scalar
  $c$ with $AX = cX$ if and only if
  \begin{equation*}
    X =
    \begin{bmatrix}
      0 \\
      0 \\
      t
    \end{bmatrix},
  \end{equation*}
  for some arbitrary scalar $t$.
\end{solution}

\Exercise5 Discover whether
\begin{equation*}
  A =
  \begin{bmatrix}
    1 & 2 & 3 & 4 \\
    0 & 2 & 3 & 4 \\
    0 & 0 & 3 & 4 \\
    0 & 0 & 0 & 4
  \end{bmatrix}
\end{equation*}
is invertible, and find $A^{-1}$ if it exists.
\begin{solution}
  We proceed in the usual way:
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3 & 4 \\
      0 & 2 & 3 & 4 \\
      0 & 0 & 3 & 4 \\
      0 & 0 & 0 & 4
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 0 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 2 & 3 & 0 \\[3pt]
      0 & 2 & 3 & 0 \\[3pt]
      0 & 0 & 3 & 0 \\[3pt]
      0 & 0 & 0 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & 0 & -1 \\[3pt]
      0 & 1 & 0 & -1 \\[3pt]
      0 & 0 & 1 & -1 \\[3pt]
      0 & 0 & 0 & \frac14
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 2 & 0 & 0 \\[3pt]
      0 & 2 & 0 & 0 \\[3pt]
      0 & 0 & 1 & 0 \\[3pt]
      0 & 0 & 0 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & 0 & -1 & 0 \\[3pt]
      0 & 1 & -1 & 0 \\[3pt]
      0 & 0 & \frac13 & -\frac13 \\[3pt]
      0 & 0 & 0 & \frac14
    \end{bmatrix} \\
    \begin{bmatrix}
      1 & 0 & 0 & 0 \\[3pt]
      0 & 1 & 0 & 0 \\[3pt]
      0 & 0 & 1 & 0 \\[3pt]
      0 & 0 & 0 & 1
    \end{bmatrix},
    &\quad
    \begin{bmatrix}
      1 & -1 & 0 & 0 \\[3pt]
      0 & \frac12 & -\frac12 & 0 \\[3pt]
      0 & 0 & \frac13 & -\frac13 \\[3pt]
      0 & 0 & 0 & \frac14
    \end{bmatrix}.
  \end{align*}
  Thus $A$ is invertible and
  \begin{equation*}
    A^{-1} =
    \begin{bmatrix}
      1 & -1 & 0 & 0 \\[3pt]
      0 & \frac12 & -\frac12 & 0 \\[3pt]
      0 & 0 & \frac13 & -\frac13 \\[3pt]
      0 & 0 & 0 & \frac14
    \end{bmatrix}. \qedhere
  \end{equation*}
\end{solution}

\Exercise6
\label{eq:lin-eq:inv:2-by-1-times-1-by-2}
Suppose $A$ is a $2\times1$ matrix and that $B$ is a $1\times2$
matrix. Prove that $C = AB$ is not invertible.
\begin{proof}
  Let
  \begin{equation*}
    A =
    \begin{bmatrix}
      a \\
      b
    \end{bmatrix}
    \quad\text{and}\quad
    B =
    \begin{bmatrix}
      c & d
    \end{bmatrix}
  \end{equation*}
  so that
  \begin{equation*}
    C = AB =
    \begin{bmatrix}
      ac & ad \\
      bc & bd
    \end{bmatrix}.
  \end{equation*}
  Suppose $C$ has an inverse. Then $C$ is row-equivalent to the
  identity matrix, and so cannot be row-equivalent to a matrix having
  a row of zeros. Consequently, each of $a$, $b$, $c$, and $d$ must be
  nonzero, since otherwise $C$ would be row-equivalent to such a
  matrix.

  But, since $a$ and $b$ are nonzero, we can multiply the second row
  of $C$ by $a/b$ to get the row-equivalent matrix
  \begin{equation*}
    \begin{bmatrix}
      ac & ad \\
      bc & bd
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      ac & ad \\
      ac & ad
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      ac & bd \\
      0 & 0
    \end{bmatrix},
  \end{equation*}
  which is clearly not invertible. Therefore $C$ cannot have an
  inverse.
\end{proof}

\Exercise7 Let $A$ be an $n\times n$ (square) matrix. Prove the
following two statements:
\begin{enumerate}
\item If $A$ is invertible and $AB = 0$ for some $n\times n$ matrix
  $B$, then $B = 0$.
  \begin{proof}
    Since $AB = 0$ and $A$ is invertible, we can multiply on the left
    by $A^{-1}$ to get
    \begin{equation*}
      B = A^{-1}0.
    \end{equation*}
    But the product on the right is clearly the $n\times n$ zero
    matrix, so $B = 0$.
  \end{proof}
\item If $A$ is not invertible, then there exists an $n\times n$
  matrix $B$ such that $AB = 0$ but $B\neq0$.
  \begin{proof}
    If $A$ is not invertible, then the homogeneous system of equations
    $AX = 0$ has a nontrivial solution $X_0$. Let $B$ be the matrix
    whose first column is $X_0$ and whose other entries are all zero,
    and consider the product $AB$.

    The entries in the first column of $AB$ must be zero since the
    first column is just $AX_0$, and the remaining entries must be
    zero since all other columns are the product of $A$ with a zero
    column. Thus the proof is complete.
  \end{proof}
\end{enumerate}

\Exercise8
\label{exercise:lin-eq:2-by-2-inv-crit}
Let
\begin{equation*}
  A =
  \begin{bmatrix}
    a & b \\
    c & d
  \end{bmatrix}.
\end{equation*}
Prove, using elementary row operations, that $A$ is invertible if and
only if
\begin{equation*}
  ad - bc \neq 0.
\end{equation*}
\begin{proof}
  First, if $A$ is invertible, then one of $a$, $b$, $c$, or $d$ must
  be nonzero. If $a\neq0$, then we can reduce
  \begin{gather*}
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      1 & \frac{b}a \\[3pt]
      c & d
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      1 & \frac{b}a \\[3pt]
      0 & \frac{ad-bc}a
    \end{bmatrix},
  \end{gather*}
  and we must have $ad - bc\neq0$ since otherwise $A$ could not be
  row-equivalent to the identity matrix, contradicting Theorem~12.

  If, instead, $a = 0$ then we must have $b\neq0$ since otherwise $A$
  would have a row of zeros and could not be row-equivalent to the
  identity matrix. So we can proceed:
  \begin{gather*}
    \begin{bmatrix}
      0 & b \\
      c & d
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      0 & 1 \\
      c & d
    \end{bmatrix}
    \xrightarrow{(3)}
    \begin{bmatrix}
      c & d \\
      0 & 1
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      c & 0 \\
      0 & 1
    \end{bmatrix},
  \end{gather*}
  and we see that we must have $c\neq0$. Thus $ad - bc =
  -bc\neq0$. This completes the first half of the proof.

  Conversely, assume that $ad - bc\neq0$. If $d\neq0$ then $A$ can be
  reduced to get
  \begin{gather*}
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      ad & bd \\
      c & d
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      ad - bc & 0 \\
      c & d
    \end{bmatrix}
    \xrightarrow{(1)} \\
    \begin{bmatrix}
      1 & 0 \\
      c & d
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      1 & 0 \\
      0 & d
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
  \end{gather*}
  and $A$ is row-equivalent to the identity matrix. On the other hand,
  if $d = 0$ then $b$ and $c$ must be nonzero and we get
  \begin{gather*}
    \begin{bmatrix}
      a & b \\
      c & 0
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      a & b \\
      1 & 0
    \end{bmatrix}
    \xrightarrow{(3)}
    \begin{bmatrix}
      1 & 0 \\
      a & b
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      1 & 0 \\
      0 & b
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
  \end{gather*}
  so that $A$ is again row-equivalent to the identity matrix. In
  either case, $A$ must be invertible by Theorem~12.
\end{proof}

\Exercise9 An $n\times n$ matrix $A$ is called {\bf upper-triangular}
if $A_{ij} = 0$ for $i > j$, that is, if every entry below the main
diagonal is $0$. Prove that an upper-triangular (square) matrix is
invertible if and only if every entry on its main diagonal is
different from $0$.
\begin{proof}
  Let $A$ be an $n\times n$ upper-triangular matrix.

  First, suppose every entry on the main diagonal of $A$ is nonzero,
  and consider the homogeneous linear system $AX = 0$:
  \begin{alignat*}{5}
    A_{11}x_1 &{}+{}& A_{12}x_2 &{}+{}& \cdots &{}+{}& A_{1n}x_n &{}={}& 0 \\
    && A_{22}x_2 &{}+{}& \cdots &{}+{}& A_{2n}x_n &{}={}& 0 \\
    && && && &\;\;\vdots \\
    && && && A_{nn}x_n &{}={}& 0\rlap.
  \end{alignat*}
  Since $A_{nn}$ is nonzero, the last equation implies that $x_n =
  0$. Then, since $A_{n-1,n-1}$ is nonzero, the second-to-last
  equation implies that $x_{n-1} = 0$. Continuing in this way, we see
  that $x_i = 0$ for each $i = 1, 2, \dots, n$. Therefore the system
  $AX = 0$ has only the trivial solution, hence $A$ is invertible.

  Conversely, suppose $A$ is invertible. Then $A$ cannot contain any
  zero rows, nor can $A$ be row-equivalent to a matrix with a row of
  zeros. This implies that $A_{nn}\neq0$. Consider $A_{n-1,n-1}$. If
  $A_{n-1,n-1}$ is zero, then by dividing row $n$ by $A_{nn}$, and
  then by adding $-A_{n-1,n}$ times row $n$ to row $n-1$, we see that
  $A$ is row-equivalent to a matrix whose $(n-1)$st row is all
  zeros. This is a contradiction, so $A_{n-1,n-1}\neq0$. In the same
  manner, we can show that $A_{ii}\neq0$ for each $i =
  1,2,\dots,n$. Thus all entries on the main diagonal of $A$ are
  nonzero.
\end{proof}

% \Exercise{10} Prove the following generalization of
% Exercise~\ref{eq:lin-eq:inv:2-by-1-times-1-by-2}. If $A$ is an
% $m\times n$ matrix, $B$ is an $n\times m$ matrix and $n < m$, then
% $AB$ is not invertible.

\Exercise{11} Let $A$ be an $m\times n$ matrix. Show that by means of
a finite number of elementary row and/or column operations one can
pass from $A$ to a matrix $R$ which is both `row-reduced echelon' and
`column-reduced echelon,' i.e., $R_{ij} = 0$ if $i\neq j$,
$R_{ii} = 1$, $1\leq i\leq r$, $R_{ii} = 0$ if $i > r$. Show that
$R = PAQ$, where $P$ is an invertible $m\times m$ matrix and $Q$ is an
invertible $n\times n$ matrix.
\begin{proof}
  By Theorem~5, $A$ is row-equivalent to a row-reduced echelon matrix
  $R_0$. And, by the second corollary to Theorem~12, there is an
  invertible $m\times m$ matrix $P$ such that $R_0 = PA$.

  Results that are analogous to Theorems~5 and 12 (with similar
  proofs) hold for column-reduced echelon matrices, so there is a
  matrix $R$ which is column-equivalent to $R_0$ and an invertible
  $n\times n$ matrix $Q$ such that $R = R_0Q$. Then $R = PAQ$ and we
  see that, through a finite number of elementary row and/or column
  operations, $A$ passes to a matrix $R$ that is both row- and
  column-reduced echelon.
\end{proof}
