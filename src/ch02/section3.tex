\section{Bases and Dimension}

\Exercise1
\label{exercise:vec-spaces:two-lin-dep-vecs}
Prove that if two vectors are linearly dependent, one of them is a
scalar multiple of the other.
\begin{proof}
  Let $\alpha_1$ and $\alpha_2$ be linearly dependent vectors in the
  space $V$. Then, by definition, there are scalars $c_1, c_2$ not
  both zero such that
  \begin{equation*}
    c_1\alpha_1 + c_2\alpha_2 = 0.
  \end{equation*}
  If $c_1$ is nonzero, then we may write
  \begin{equation*}
    \alpha_1 = -\frac{c_2}{c_1}\alpha_2
  \end{equation*}
  so that $\alpha_1$ is a scalar multiple of $\alpha_2$. If $c_1 = 0$,
  then $c_2$ is nonzero and a similar argument will do.
\end{proof}

\Exercise2
\label{exercise:vec-spaces:vecs-in-R4}
Are the  vectors
\begin{align*}
  \alpha_1 &= (1, 1, 2, 4), \quad \alpha_2 = (2, -1, -5, 2) \\
  \alpha_3 &= (1, -1, -4, 0), \quad \alpha_4 = (2, 1, 1, 6)
\end{align*}
linearly independent in $R^4$?
\begin{solution}
  Suppose $c_1\alpha_1 + c_2\alpha_2 + c_3\alpha_3 + c_4\alpha_4 =
  0$. This leads to the system of equations
  \begin{alignat*}{5}
    c_1 &{}+{}& 2c_2 &{}+{}& c_3 &{}+{}& 2c_4 &{}={}& 0 \\
    c_1 &{}-{}& c_2 &{}-{}& c_3 &{}+{}& c_4 &{}={}& 0 \\
    2c_1 &{}-{}& 5c_2 &{}-{}& 4c_3 &{}+{}& c_4 &{}={}& 0 \\
    4c_1 &{}+{}& 2c_2 && &{}+{}& 6c_4 &{}={}& 0\rlap.
  \end{alignat*}
  Using the method of elimination developed in Chapter~1, we find that
  this system has the general solution
  \begin{equation*}
    (c_1,c_2,c_3,c_4) = \left(\frac{s - 4t}3, \frac{-2s - t}3, s, t\right),
  \end{equation*}
  where $s,t\in R^4$ are arbitrary. For example, we may take $s = 3$
  and $t = 0$ to get $c_1 = 1$, $c_2 = -2$, $c_3 = 3$, and $c_4 =
  0$. This shows that the vectors
  $\alpha_1,\alpha_2,\alpha_3,\alpha_4$ are linearly dependent.
\end{solution}

\Exercise3 Find a basis for the subspace of $R^4$ spanned by the four
vectors of Exercise~\ref{exercise:vec-spaces:vecs-in-R4}.
\begin{solution}
  Since $\alpha_2$ is not a scalar multiple of $\alpha_1$, the set
  $\{\alpha_1,\alpha_2\}$ is linearly independent (by
  Exercise~\ref{exercise:vec-spaces:two-lin-dep-vecs}). We also see
  that it spans the subspace since we can write
  \begin{equation*}
    \alpha_3 = \frac23\alpha_2 - \frac13\alpha_1
  \end{equation*}
  and
  \begin{equation*}
    \alpha_4 = \frac43\alpha_1 + \frac13\alpha_2.
  \end{equation*}
  So $\{\alpha_1,\alpha_2\}$ is a basis for the subspace.
\end{solution}

\Exercise4 Show that the vectors
\begin{equation*}
  \alpha_1 = (1, 0, -1), \quad \alpha_2 = (1, 2, 1),
  \quad \alpha_3 = (0, -3, 2)
\end{equation*}
form a basis for $R^3$. Express each of the standard basis vectors as
linear combinations of $\alpha_1$, $\alpha_2$, and $\alpha_3$.
\begin{solution}
  Since $\dim R^3 = 3$, we need only show that the three vectors are
  independent. Let $c_1,c_2,c_3$ be scalars such that
  \begin{equation*}
    c_1\alpha_1 + c_2\alpha_2 + c_3\alpha_3 = 0.
  \end{equation*}
  Then we arrive at the homogeneous system of equations $Ax = 0$,
  where $A$ is the $3\times3$ matrix whose $j$th column is
  $\alpha_j$. By row-reducing this matrix, we see that it is
  row-equivalent to the identity matrix. Hence the system $Ax = 0$ has
  only the trivial solution, i.e. $c_1 = c_2 = c_3 = 0$. Therefore
  $\{\alpha_1, \alpha_2, \alpha_3\}$ is a basis for $R^3$.

  To write the standard basis vectors as linear combinations of
  $\alpha_1,\alpha_2,\alpha_3$, we may solve the systems
  $Ax = \epsilon_i$. This gives
  \begin{align*}
    (1, 0, 0) &= \frac{7}{10}\alpha_1 + \frac3{10}\alpha_2 + \frac15\alpha_3 \\[3pt]
    (0, 1, 0) &= -\frac15\alpha_1 + \frac15\alpha_2 - \frac15\alpha_3 \\[3pt]
    (0, 0, 1) &= -\frac3{10}\alpha_1 + \frac3{10}\alpha_2 + \frac15\alpha_3.\qedhere
  \end{align*}
\end{solution}

\Exercise5 Find three vectors in $R^3$ which are linearly dependent,
and are such that any two of them are linearly independent.
\begin{solution}
  Consider the vectors
  \begin{equation*}
    \alpha_1 = (1, 0, 0), \quad
    \alpha_2 = (0, 1, 0), \quad\text{and}\quad
    \alpha_3 = (1, 1, 0).
  \end{equation*}
  These vectors are pairwise-independent since neither is a scalar
  multiple of another. But they are clearly linearly dependent since
  $\alpha_1 + \alpha_2 - \alpha_3 = 0$.
\end{solution}

\Exercise6
\label{exercise:vec-spaces:F2x2-basis}
Let $V$ be the vector space of all $2\times2$ matrices over the field
$F$. Prove that $V$ has dimension $4$ by exhibiting a basis for $V$
which has four elements.
\begin{solution}
  We may simply take the standard basis:
  \begin{equation*}
    \left\{
      \begin{bmatrix}
        1 & 0 \\ 0 & 0
      \end{bmatrix},
      \begin{bmatrix}
        0 & 1 \\ 0 & 0
      \end{bmatrix},
      \begin{bmatrix}
        0 & 0 \\ 1 & 0
      \end{bmatrix},
      \begin{bmatrix}
        0 & 0 \\ 0 & 1
      \end{bmatrix}
    \right\}. \qedhere
  \end{equation*}
\end{solution}

\Exercise7 Let $V$ be the vector space of Exercise~6. Let $W_1$ be the
set of matrices of the form
\begin{equation*}
  \begin{bmatrix}
    x & -x \\ y & z
  \end{bmatrix}
\end{equation*}
and let $W_2$ be the set of matrices of the form
\begin{equation*}
  \begin{bmatrix}
    a & b \\ -a & c
  \end{bmatrix}.
\end{equation*}
\begin{enumerate}
\item Prove that $W_1$ and $W_2$ are subspaces of $V$.
  \begin{proof}
    Both sets are nonempty. Consider the arbitrary matrices
    \begin{equation*}
      A_1 =
      \begin{bmatrix}
        x_1 & -x_1 \\ y_1 & z_1
      \end{bmatrix}
      \quad\text{and}\quad
      A_2 =
      \begin{bmatrix}
        x_2 & -x_2 \\ y_2 & z_2
      \end{bmatrix}
    \end{equation*}
    in $W_1$ and let $c$ be an arbitrary scalar. Then
    \begin{equation*}
      cA_1 + A_2 =
      \begin{bmatrix}
        cx_1 + x_2 & -cx_1 - x_2 \\ y_1 + y_2 & z_1 + z_2
      \end{bmatrix}
      =
      \begin{bmatrix}
        cx_1 + x_2 & -(cx_1 + x_2) \\ y_1 + y_2 & z_1 + z_2
      \end{bmatrix}
    \end{equation*}
    which is again in $W_1$. This shows that $W_1$ is a subspace of
    $V$.

    A similar argument will show that $W_2$ is a subspace of $V$.
  \end{proof}

\item Find the dimensions of $W_1$, $W_2$, $W_1 + W_2$, and
  $W_1\cap W_2$.
  \begin{solution}
    First we find bases for $W_1$ and $W_2$. We may take
    \begin{equation*}
      \left\{
        \begin{bmatrix}
          1 & -1 \\ 0 & 0
        \end{bmatrix},
        \begin{bmatrix}
          0 & 0 \\ 1 & 0
        \end{bmatrix},
        \begin{bmatrix}
          0 & 0 \\ 0 & 1
        \end{bmatrix}
      \right\}
    \end{equation*}
    as a basis for $W_1$ and
    \begin{equation*}
      \left\{
        \begin{bmatrix}
          1 & 0 \\ -1 & 0
        \end{bmatrix},
        \begin{bmatrix}
          0 & 1 \\ 0 & 0
        \end{bmatrix},
        \begin{bmatrix}
          0 & 0 \\ 0 & 1
        \end{bmatrix}
      \right\}
    \end{equation*}
    as a basis for $W_2$. Consequently, we see that
    $\dim W_1 = \dim W_2 = 3$.

    Next, observe that matrices in $W_1\cap W_2$ must have the form
    \begin{equation*}
      \begin{bmatrix}
        x & -x \\ -x & y
      \end{bmatrix}.
    \end{equation*}
    A basis for this space is then
    \begin{equation*}
      \left\{
        \begin{bmatrix}
          1 & -1 \\ -1 & 0
        \end{bmatrix},
        \begin{bmatrix}
          0 & 0 \\ 0 & 1
        \end{bmatrix}
      \right\}
    \end{equation*}
    so that $\dim(W_1 \cap W_2) = 2$. Finally, we may apply Theorem~6
    to determine that
    \begin{equation*}
      \dim(W_1 + W_2) = \dim W_1 + \dim W_2 - \dim(W_1\cap W_2)
      = 3 + 3 - 2 = 4.
    \end{equation*}
    It follows that $W_1 + W_2 = V$.
  \end{solution}
\end{enumerate}

\Exercise8 Again let $V$ be the space of $2\times2$ matrices over
$F$. Find a basis $\{A_1, A_2, A_3, A_4\}$ for $V$ such that
$A_j^2 = A_j$ for each $j$.
\begin{solution}
  Let
  \begin{equation*}
    A_1 =
    \begin{bmatrix}
      1 & 0 \\ 0 & 0
    \end{bmatrix},
    \quad
    A_2 =
    \begin{bmatrix}
      0 & 0 \\ 0 & 1
    \end{bmatrix},
    \quad
    A_3 =
    \begin{bmatrix}
      1 & 1 \\ 0 & 0
    \end{bmatrix},
    \quad\text{and}\quad
    \begin{bmatrix}
      0 & 0 \\ 1 & 1
    \end{bmatrix}.
  \end{equation*}
  A simple check will show that $A_j^2 = A_j$ for each $j$. To show
  that $\{A_1, A_2, A_3, A_4\}$ is a basis for $V$, we need only show
  that it spans $V$ (since any spanning set with four vectors must be
  linearly independent).

  Let
  \begin{equation*}
    B =
    \begin{bmatrix}
      x & y \\
      z & w
    \end{bmatrix}
  \end{equation*}
  be an arbitrary $2\times2$ matrix over $F$. Then we can write $B$ as
  a linear combination of $A_1,A_2,A_3,A_4$ as follows:
  \begin{equation*}
    B = (x - y)A_1 + (w - z)A_2 + yA_3 + zA_4.
  \end{equation*}
  Therefore the set $\{A_1, A_2, A_3, A_4\}$ is indeed a basis for
  $V$.
\end{solution}

\Exercise9 Let $V$ be a vector space over a subfield $F$ of the
complex numbers. Suppose $\alpha$, $\beta$, and $\gamma$ are linearly
independent vectors in $V$. Prove that $(\alpha + \beta)$,
$(\beta + \gamma)$, and $(\gamma + \alpha)$ are linearly independent.
\begin{proof}
  Let $c_1$, $c_2$, and $c_3$ be scalars in $F$ such that
  \begin{equation*}
    c_1(\alpha + \beta) + c_2(\beta + \gamma) + c_3(\gamma + \alpha) = 0.
  \end{equation*}
  By rearranging, this becomes
  \begin{equation*}
    (c_1 + c_3)\alpha + (c_1 + c_2)\beta + (c_2 + c_3)\gamma = 0.
  \end{equation*}
  Since $\alpha$, $\beta$, and $\gamma$ are linearly independent, we
  must have
  \begin{equation*}
    c_1 + c_3 = 0, \quad c_1 + c_2 = 0,
    \quad\text{and}\quad c_2 + c_3 = 0.
  \end{equation*}
  But this system of equations has the unique solution
  $(c_1,c_2,c_3) = (0, 0, 0)$. Therefore $(\alpha + \beta)$,
  $(\beta + \gamma)$, and $(\gamma + \alpha)$ are linearly
  independent.
\end{proof}
